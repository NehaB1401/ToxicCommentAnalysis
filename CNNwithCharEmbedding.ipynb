{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "To build a multi-headed model that’s capable of detecting different types of of toxicity like threats, obscenity, insults and identity-based hate. The dataset used are comments from Wikipedia’s talk page edits.\n",
    "\n",
    "Google built a range of publicly available models served through the Perspective API, including toxicity. But the current models still make errors, and they don’t allow users to select which types of toxicity they’re interested in finding (e.g. some platforms may be fine with profanity, but not with other types of toxic content).\n",
    "\n",
    "We have extended our neural network model further to prove that this model is able to detect toxicity level and also rightly classify the toxicity level which is not provided by Google's Perspective API. Below is an example of an input test comment \"I will kill you\" and it has been rightly classified by this model as 95% threat.\n",
    "\n",
    "We then compared our model outputs with that of Google's Perspective API's model with various input sentences.\n",
    "\n",
    "We further intend on extending our model to run over twitter comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach\n",
    "\n",
    "The idea is to use the train dataset to train the models with the words in the comments as predictor variables and to predict the probability of toxicity level of a comment. A pre- trained model that gives the best accuracy to various user comments to combat the ongoing issue of online forum abuse. Our project is focused on developing a series of neural network models. The goal is to find the strengths and weakness of different Deep Learning models on the text classification task. We developed 3 specific Neural Network models for this project which are as follows:\n",
    "1. Convolution Neural Network (CNN with character-level embedding)\n",
    "2. Convolution Neural Network (CNN with word embedding)\n",
    "3. Recurrent Neural Network (RNN) with Long Short Term Memory (LSTM) cells\n",
    "We intend to test the above models trained with the Wikipedia data and word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic Comment Classification\n",
    "\n",
    "In this notebook, we'll be developing a Neural Network models that can classify string comments based on their toxicity:\n",
    "* `toxic`\n",
    "* `severe_toxic`\n",
    "* `obscene`\n",
    "* `threat`\n",
    "* `insult`\n",
    "* `identity_hate`\n",
    "\n",
    "This is a part of the [Toxic Comment Classification](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) Kaggle competition. From the site:\n",
    "\n",
    ">In this competition, you’re challenged to build a multi-headed model that’s capable of detecting different types of toxicity like threats, obscenity, insults, and identity-based hate better than Perspective’s current models. You’ll be using a dataset of comments from Wikipedia’s talk page edits. Improvements to the current model will hopefully help online discussion become more productive and respectful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data\n",
    "The data we'll be using consists of a large number of Wikipedia comments which have been labeled by humans according to their relative toxicity. The data can be found here. Download the following and store in directory's data folder.\n",
    "\n",
    "    train.csv - the training set, contains comments with their binary labels.\n",
    "    test.csv - the test set, predict toxicity probabilities for these comments.\n",
    "    sample_submission.csv - the submission sample with the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function for ploting history of the model training\n",
    "from pyspark.sql import Row\n",
    "def plot_history(history_arg):\n",
    "  array = []\n",
    "  i =1\n",
    "  j =1\n",
    "  for acc in history_arg.history['acc']:\n",
    "    array.append(Row(epoch=i, accuracy=float(acc)))\n",
    "    i = i+1\n",
    "  acc_df = sqlContext.createDataFrame((array))\n",
    "\n",
    "  array = []\n",
    "  for loss in history_arg.history['loss']:\n",
    "      array.append(Row(epoch = j, loss = float(loss)))\n",
    "      j = j+1\n",
    "  loss_df = sqlContext.createDataFrame(array)\n",
    "\n",
    "  display_df = acc_df.join(loss_df,on=(\"epoch\")).orderBy(\"epoch\")\n",
    "  return display_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Data\n",
    "Extract the features and labels and take a look at sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "#Class labels\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "#Read the data\n",
    "toxicWordsTrain = pd.read_csv(\"train.csv\");\n",
    "toxicWordsTest = pd.read_csv(\"test.csv\")\n",
    "\n",
    "y_train = toxicWordsTrain[list_classes].values\n",
    "x_train = toxicWordsTrain[\"comment_text\"]\n",
    "x_test  = toxicWordsTest[\"comment_text\"]\n",
    "\n",
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment #1:  Explanation\n",
      "Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\n",
      "Label #1:    [0 0 0 0 0 0]\n",
      "Comment #2:  D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)\n",
      "Label #2:    [0 0 0 0 0 0]\n",
      "Comment #3:  Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.\n",
      "Label #3:    [0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Sample from dataset\n",
    "for sample_i in range(3):\n",
    "    print('Comment #{}:  {}'.format(sample_i + 1, x_train[sample_i]))\n",
    "    print('Label #{}:    {}'.format(sample_i + 1, y_train[sample_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 159571/159571 [00:02<00:00, 58681.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10734904 words.\n",
      "532299 unique words.\n",
      "10 Most common words in the dataset:\n",
      "\"the\" \"to\" \"of\" \"and\" \"a\" \"I\" \"is\" \"you\" \"that\" \"in\"\n"
     ]
    }
   ],
   "source": [
    "# Explore vocabulary\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create a counter object for each dataset\n",
    "word_counter = collections.Counter([word for sentence in tqdm(x_train, total=len(x_train)) \\\n",
    "                                                              for word in sentence.split()])\n",
    "\n",
    "print('{} words.'.format(len([word for sentence in x_train for word in sentence.split()])))\n",
    "print('{} unique words.'.format(len(word_counter)))\n",
    "print('10 Most common words in the dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*word_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 10,734,904 words, 532,299 of which are unique, and the 10 most common being: \"the\", \"to\", \"of\", \"and\", \"a\", \"I\", \"is\", \"you\", \"that\", and \"in\". One problem here is that we are counting uppercase words as different from lower case words and a bunch of other symbols that aren't really useful for our goal. A data cleanup will be done in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data\n",
    "We preprocess our data a bit so that it's in a format we can input into a neural network. The process includes:\n",
    "\n",
    "    1. Remove irrelevant characters (!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n).\n",
    "    2. Convert all letters to lowercase (HeLlO -> hello).\n",
    "    3. As this is character level embedding, Consider every character as a seperate token. \n",
    "    4. Tokenize our words (hi how are you -> [23, 1, 5, 13]).\n",
    "    5. Standaridize our input length with padding (hi how are you -> [23, 1, 5, 13, 0, 0, 0]).\n",
    "We can go further and consider combining misspelled, slang, or different word inflections into single base words. However, the benefit of using a neural network is that they do well with raw input, so we'll stick with what we have listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 2335\n",
      "Longest comment size: 5000\n",
      "Average comment size: 394.0732213246768\n",
      "Stdev of comment size: 590.7184309382144\n",
      "Max comment size: 2166\n",
      "\n",
      "Sequence 1\n",
      "  Input:  Explanation\n",
      "Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\n",
      "  Output: [35, 48, 17, 11, 4, 7, 4, 3, 6, 5, 7, 26, 33, 10, 16, 1, 3, 10, 2, 1, 2, 12, 6, 3, 8, 1, 15, 4, 12, 2, 1, 13, 7, 12, 2, 9, 1, 15, 16, 1, 13, 8, 2, 9, 7, 4, 15, 2, 1, 39, 4, 9, 12, 14, 5, 9, 2, 1, 46, 2, 3, 4, 11, 11, 6, 14, 4, 1, 54, 4, 7, 1, 20, 2, 9, 2, 1, 9, 2, 24, 2, 9, 3, 2, 12, 56, 1, 29, 10, 2, 16, 1, 20, 2, 9, 2, 7, 30, 3, 1, 24, 4, 7, 12, 4, 11, 6, 8, 15, 8, 25, 1, 57, 13, 8, 3, 1, 14, 11, 5, 8, 13, 9, 2, 1, 5, 7, 1, 8, 5, 15, 2, 1, 58, 31, 8, 1, 4, 19, 3, 2, 9, 1, 27, 1, 24, 5, 3, 2, 12, 1, 4, 3, 1, 38, 2, 20, 1, 59, 5, 9, 23, 1, 50, 5, 11, 11, 8, 1, 54, 31, 34, 22, 1, 31, 7, 12, 1, 17, 11, 2, 4, 8, 2, 1, 12, 5, 7, 30, 3, 1, 9, 2, 15, 5, 24, 2, 1, 3, 10, 2, 1, 3, 2, 15, 17, 11, 4, 3, 2, 1, 19, 9, 5, 15, 1, 3, 10, 2, 1, 3, 4, 11, 23, 1, 17, 4, 18, 2, 1, 8, 6, 7, 14, 2, 1, 27, 30, 15, 1, 9, 2, 3, 6, 9, 2, 12, 1, 7, 5, 20, 22, 70, 64, 22, 52, 43, 67, 22, 65, 70, 22, 52, 72]\n",
      "Sequence 2\n",
      "  Input:  D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)\n",
      "  Output: [50, 30, 4, 20, 20, 40, 1, 39, 2, 1, 15, 4, 3, 14, 10, 2, 8, 1, 3, 10, 6, 8, 1, 21, 4, 14, 23, 18, 9, 5, 13, 7, 12, 1, 14, 5, 11, 5, 13, 9, 1, 27, 30, 15, 1, 8, 2, 2, 15, 6, 7, 18, 11, 16, 1, 8, 3, 13, 14, 23, 1, 20, 6, 3, 10, 22, 1, 29, 10, 4, 7, 23, 8, 22, 1, 1, 51, 3, 4, 11, 23, 47, 1, 52, 41, 45, 67, 41, 25, 1, 66, 4, 7, 13, 4, 9, 16, 1, 41, 41, 25, 1, 52, 43, 41, 71, 1, 51, 53, 29, 34, 47]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and Pad\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = Tokenizer(num_words=None,\n",
    "                      filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                      lower=True,\n",
    "                      split=\" \",\n",
    "                      char_level=True)\n",
    "\n",
    "# Fit and run tokenizer\n",
    "tokenizer.fit_on_texts(list(x_train))\n",
    "tokenized_train = tokenizer.texts_to_sequences(x_train)\n",
    "tokenized_test = tokenizer.texts_to_sequences(x_test)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Extract variables\n",
    "vocab_size = len(word_index)\n",
    "print('Vocab size: {}'.format(vocab_size))\n",
    "longest = max(len(seq) for seq in tokenized_train)\n",
    "print(\"Longest comment size: {}\".format(longest))\n",
    "average = np.mean([len(seq) for seq in tokenized_train])\n",
    "print(\"Average comment size: {}\".format(average))\n",
    "stdev = np.std([len(seq) for seq in tokenized_train])\n",
    "print(\"Stdev of comment size: {}\".format(stdev))\n",
    "max_len = int(average + stdev * 3)\n",
    "print('Max comment size: {}'.format(max_len))\n",
    "print()\n",
    "\n",
    "# Pad sequences\n",
    "processed_X_train = pad_sequences(tokenized_train, maxlen=max_len, padding='post', truncating='post')\n",
    "processed_X_test = pad_sequences(tokenized_test, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# Sample tokenization\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(x_train[:2], tokenized_train[:2])):\n",
    "    print('Sequence {}'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(sent))\n",
    "    print('  Output: {}'.format(token_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preprocessing, our vocabulary size drops to a more manageable 210,337 with a max comment size of 371 words and an average comment size of about 68 words per sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding\n",
    "The data representation for our vocabulary is one-hot encoding where every word is transformed into a vector with a 1 in its corresponding location. For example, if our word vector is [hi, how, are, you] and the word we are looking at is \"you\", the input vector for \"you\" would just be [0, 0, 0, 1]. This works fine unless our vocabulary is huge - in this case, 210,000 - which means we would end up with word vectors that consist mainly of a bunch of 0s.\n",
    "\n",
    "Instead, we can use a Word2Vec technique to find continuous embeddings for our words. Here, we'll be using the pretrained FastText embeddings from Facebook to produce a 300-dimension vector for each word in our vocabulary.\n",
    "\n",
    "    P. Bojanowski, E. Grave, A. Joulin, T. Mikolov, Enriching Word Vectors with Subword Information\n",
    "The benefit of this continuous embedding is that words with similar predictive power will appear closer together on our word vector. The downside is that this creates more of a black box where the words with the most predictive power get lost in the numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2519371 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 300\n",
    "\n",
    "# Get embeddings\n",
    "embeddings_index = {}\n",
    "f = open('wiki.en.vec', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.rstrip().rsplit(' ', embedding_dim)\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found {} word vectors.'.format(len(embeddings_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build embedding matrix\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is a lengthy process and hence saving the output to the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save embeddings\n",
    "import h5py\n",
    "with h5py.File('embeddings.h5', 'w') as hf:\n",
    "    hf.create_dataset(\"fasttext\",  data=embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "with h5py.File('embeddings.h5', 'r') as hf:\n",
    "    embedding_matrix = hf['fasttext'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Now that the data is preprocessed and our embeddings are ready, we can build a model. We will build a neural network architecture that is comprises of the following:\n",
    "\n",
    "    Embedding layer - word vector representations.\n",
    "    Convolutional layer - run multiple filters over that temporal data.\n",
    "    Fully connected layer - classify input based on filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nehab\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 2166, 300)         700800    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 2166, 128)         192128    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 722, 128)          0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                6450      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 900,196\n",
      "Trainable params: 899,940\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras.backend\n",
    "from keras.models import Sequential\n",
    "from keras.layers import CuDNNGRU, Dense, Conv1D, MaxPooling1D\n",
    "from keras.layers import Dropout, GlobalMaxPooling1D, BatchNormalization\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Nadam\n",
    "\n",
    "# Initate model\n",
    "model = Sequential()\n",
    "\n",
    "# Add Embedding layer\n",
    "model.add(Embedding(vocab_size + 1, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=True))\n",
    "\n",
    "# Add Recurrent layers\n",
    "#model.add(Bidirectional(CuDNNGRU(300, return_sequences=True)))\n",
    "\n",
    "# Add Convolutional layer\n",
    "model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Add fully connected layers\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(6, activation='sigmoid'))\n",
    "\n",
    "# Summarize the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile the model\n",
    "We'll be using binary crossentropy as our loss function and clipping our gradients to avoid any explosions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(y_true, y_pred):\n",
    "     return keras.backend.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "lr = .0001\n",
    "model.compile(loss=loss, optimizer=Nadam(lr=lr, clipnorm=1.0),\n",
    "              metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metric\n",
    "To evaluate our model, we'll be looking at its AUC ROC score (area under the receiver operating characteristic curve). We will be looking at the probability that our model ranks a randomly chosen positive instance higher than a randomly chosen negative one. With data that mostly consists of negative labels (no toxicity), our model could just learn to always predict negative and end up with a pretty high accuracy. AUC ROC helps correct this by putting more weight on the the positive examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After init\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, filepath, validation_data=(), interval=1, max_epoch = 100):\n",
    "        super(Callback, self).__init__()\n",
    "        # Initialize state variables\n",
    "        print(\"After init\")\n",
    "        self.interval = interval\n",
    "        self.filepath = filepath\n",
    "        self.stopped_epoch = max_epoch\n",
    "        self.best = 0\n",
    "        self.X_val, self.y_val = validation_data\n",
    "        self.y_pred = np.zeros(self.y_val.shape)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        print(\"Epoch end 1\")\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict_proba(self.X_val, verbose=0)\n",
    "            current = roc_auc_score(self.y_val, y_pred)\n",
    "            logs['roc_auc_val'] = current\n",
    "\n",
    "            if current > self.best: #save model\n",
    "                print(\" - AUC - improved from {:.5f} to {:.5f}\".format(self.best, current))\n",
    "                self.best = current\n",
    "                self.y_pred = y_pred\n",
    "                self.stopped_epoch = epoch+1\n",
    "                self.model.save(self.filepath, overwrite=True)\n",
    "            else:\n",
    "                print(\" - AUC - did not improve\")\n",
    "            \n",
    "[X, X_val, y, y_val] = train_test_split(processed_X_train, y_train, test_size=0.03, shuffle=False)\n",
    "RocAuc = RocAucEvaluation(filepath='model.best.hdf5',validation_data=(X_val, y_val), interval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "Using a batch size of 64 and set the number of epochs as 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 154783 samples, validate on 4788 samples\n",
      "Epoch 1/1\n",
      " 18496/154783 [==>...........................] - ETA: 6:26:48 - loss: 0.86 - ETA: 5:02:30 - loss: 0.88 - ETA: 4:25:14 - loss: 0.87 - ETA: 4:00:30 - loss: 0.86 - ETA: 3:45:30 - loss: 0.85 - ETA: 3:34:46 - loss: 0.83 - ETA: 3:26:33 - loss: 0.82 - ETA: 3:19:38 - loss: 0.80 - ETA: 3:16:21 - loss: 0.79 - ETA: 3:12:09 - loss: 0.78 - ETA: 3:08:48 - loss: 0.77 - ETA: 3:06:02 - loss: 0.75 - ETA: 3:03:33 - loss: 0.74 - ETA: 3:02:07 - loss: 0.73 - ETA: 3:00:21 - loss: 0.71 - ETA: 2:58:41 - loss: 0.70 - ETA: 2:57:04 - loss: 0.69 - ETA: 2:55:55 - loss: 0.68 - ETA: 2:54:42 - loss: 0.67 - ETA: 2:53:44 - loss: 0.66 - ETA: 2:52:41 - loss: 0.65 - ETA: 2:51:55 - loss: 0.65 - ETA: 2:51:02 - loss: 0.64 - ETA: 2:50:19 - loss: 0.63 - ETA: 2:49:35 - loss: 0.62 - ETA: 2:48:52 - loss: 0.61 - ETA: 2:48:13 - loss: 0.61 - ETA: 2:47:40 - loss: 0.60 - ETA: 2:47:03 - loss: 0.59 - ETA: 2:46:26 - loss: 0.58 - ETA: 2:45:54 - loss: 0.58 - ETA: 2:45:24 - loss: 0.57 - ETA: 2:44:54 - loss: 0.56 - ETA: 2:44:34 - loss: 0.56 - ETA: 2:44:12 - loss: 0.55 - ETA: 2:44:38 - loss: 0.54 - ETA: 2:44:41 - loss: 0.54 - ETA: 2:44:52 - loss: 0.54 - ETA: 2:44:56 - loss: 0.53 - ETA: 2:47:02 - loss: 0.53 - ETA: 2:47:47 - loss: 0.52 - ETA: 2:47:28 - loss: 0.52 - ETA: 2:47:02 - loss: 0.51 - ETA: 2:46:37 - loss: 0.50 - ETA: 2:46:11 - loss: 0.50 - ETA: 2:45:48 - loss: 0.50 - ETA: 2:45:23 - loss: 0.49 - ETA: 2:44:56 - loss: 0.49 - ETA: 2:44:30 - loss: 0.48 - ETA: 2:44:09 - loss: 0.48 - ETA: 2:43:44 - loss: 0.47 - ETA: 2:43:21 - loss: 0.47 - ETA: 2:42:58 - loss: 0.46 - ETA: 2:42:37 - loss: 0.46 - ETA: 2:42:16 - loss: 0.46 - ETA: 2:41:53 - loss: 0.45 - ETA: 2:41:35 - loss: 0.45 - ETA: 2:41:59 - loss: 0.45 - ETA: 2:42:15 - loss: 0.44 - ETA: 2:41:58 - loss: 0.44 - ETA: 2:41:45 - loss: 0.43 - ETA: 2:41:30 - loss: 0.43 - ETA: 2:41:13 - loss: 0.43 - ETA: 2:41:01 - loss: 0.42 - ETA: 2:40:45 - loss: 0.42 - ETA: 2:40:28 - loss: 0.42 - ETA: 2:40:10 - loss: 0.42 - ETA: 2:39:54 - loss: 0.41 - ETA: 2:39:37 - loss: 0.41 - ETA: 2:39:28 - loss: 0.41 - ETA: 2:39:12 - loss: 0.41 - ETA: 2:38:58 - loss: 0.40 - ETA: 2:38:45 - loss: 0.40 - ETA: 2:38:34 - loss: 0.40 - ETA: 2:38:22 - loss: 0.39 - ETA: 2:38:13 - loss: 0.39 - ETA: 2:38:01 - loss: 0.39 - ETA: 2:38:10 - loss: 0.39 - ETA: 2:39:00 - loss: 0.38 - ETA: 2:39:15 - loss: 0.38 - ETA: 2:39:28 - loss: 0.38 - ETA: 2:39:33 - loss: 0.38 - ETA: 2:39:43 - loss: 0.37 - ETA: 2:39:48 - loss: 0.37 - ETA: 2:39:35 - loss: 0.37 - ETA: 2:39:21 - loss: 0.37 - ETA: 2:39:09 - loss: 0.36 - ETA: 2:38:59 - loss: 0.36 - ETA: 2:38:46 - loss: 0.36 - ETA: 2:38:33 - loss: 0.36 - ETA: 2:38:20 - loss: 0.36 - ETA: 2:38:08 - loss: 0.36 - ETA: 2:37:55 - loss: 0.35 - ETA: 2:37:44 - loss: 0.35 - ETA: 2:37:34 - loss: 0.35 - ETA: 2:37:24 - loss: 0.35 - ETA: 2:37:14 - loss: 0.35 - ETA: 2:37:02 - loss: 0.35 - ETA: 2:36:50 - loss: 0.34 - ETA: 2:36:41 - loss: 0.34 - ETA: 2:36:32 - loss: 0.34 - ETA: 2:36:22 - loss: 0.34 - ETA: 2:36:10 - loss: 0.34 - ETA: 2:36:00 - loss: 0.33 - ETA: 2:35:52 - loss: 0.33 - ETA: 2:35:41 - loss: 0.33 - ETA: 2:35:30 - loss: 0.33 - ETA: 2:35:21 - loss: 0.33 - ETA: 2:35:12 - loss: 0.33 - ETA: 2:35:01 - loss: 0.32 - ETA: 2:34:50 - loss: 0.32 - ETA: 2:34:45 - loss: 0.32 - ETA: 2:34:42 - loss: 0.32 - ETA: 2:34:42 - loss: 0.32 - ETA: 2:34:39 - loss: 0.32 - ETA: 2:34:31 - loss: 0.32 - ETA: 2:34:22 - loss: 0.31 - ETA: 2:34:14 - loss: 0.31 - ETA: 2:34:06 - loss: 0.31 - ETA: 2:34:00 - loss: 0.31 - ETA: 2:33:55 - loss: 0.31 - ETA: 2:33:51 - loss: 0.31 - ETA: 2:33:41 - loss: 0.31 - ETA: 2:33:31 - loss: 0.30 - ETA: 2:33:22 - loss: 0.30 - ETA: 2:33:13 - loss: 0.30 - ETA: 2:33:07 - loss: 0.30 - ETA: 2:33:01 - loss: 0.30 - ETA: 2:32:52 - loss: 0.30 - ETA: 2:32:50 - loss: 0.30 - ETA: 2:32:46 - loss: 0.29 - ETA: 2:32:37 - loss: 0.29 - ETA: 2:32:30 - loss: 0.29 - ETA: 2:32:21 - loss: 0.29 - ETA: 2:32:19 - loss: 0.29 - ETA: 2:32:10 - loss: 0.29 - ETA: 2:32:08 - loss: 0.29 - ETA: 2:32:01 - loss: 0.29 - ETA: 2:31:53 - loss: 0.29 - ETA: 2:31:48 - loss: 0.28 - ETA: 2:31:42 - loss: 0.28 - ETA: 2:31:35 - loss: 0.28 - ETA: 2:31:33 - loss: 0.28 - ETA: 2:31:28 - loss: 0.28 - ETA: 2:31:22 - loss: 0.28 - ETA: 2:31:20 - loss: 0.28 - ETA: 2:31:15 - loss: 0.28 - ETA: 2:31:09 - loss: 0.28 - ETA: 2:31:04 - loss: 0.28 - ETA: 2:31:03 - loss: 0.27 - ETA: 2:30:57 - loss: 0.27 - ETA: 2:30:51 - loss: 0.27 - ETA: 2:30:47 - loss: 0.27 - ETA: 2:30:45 - loss: 0.27 - ETA: 2:30:40 - loss: 0.27 - ETA: 2:30:31 - loss: 0.27 - ETA: 2:30:30 - loss: 0.27 - ETA: 2:30:24 - loss: 0.27 - ETA: 2:30:19 - loss: 0.27 - ETA: 2:30:15 - loss: 0.26 - ETA: 2:30:14 - loss: 0.26 - ETA: 2:30:09 - loss: 0.26 - ETA: 2:30:08 - loss: 0.26 - ETA: 2:30:01 - loss: 0.26 - ETA: 2:30:00 - loss: 0.26 - ETA: 2:29:52 - loss: 0.26 - ETA: 2:29:45 - loss: 0.26 - ETA: 2:29:39 - loss: 0.26 - ETA: 2:29:31 - loss: 0.26 - ETA: 2:29:23 - loss: 0.26 - ETA: 2:29:16 - loss: 0.26 - ETA: 2:29:10 - loss: 0.26 - ETA: 2:29:03 - loss: 0.25 - ETA: 2:28:56 - loss: 0.25 - ETA: 2:28:49 - loss: 0.25 - ETA: 2:28:42 - loss: 0.25 - ETA: 2:28:35 - loss: 0.25 - ETA: 2:28:29 - loss: 0.25 - ETA: 2:28:23 - loss: 0.25 - ETA: 2:28:17 - loss: 0.25 - ETA: 2:28:11 - loss: 0.25 - ETA: 2:28:04 - loss: 0.25 - ETA: 2:27:57 - loss: 0.25 - ETA: 2:27:50 - loss: 0.24 - ETA: 2:27:43 - loss: 0.24 - ETA: 2:27:36 - loss: 0.24 - ETA: 2:27:29 - loss: 0.24 - ETA: 2:27:23 - loss: 0.24 - ETA: 2:27:16 - loss: 0.24 - ETA: 2:27:09 - loss: 0.24 - ETA: 2:27:01 - loss: 0.24 - ETA: 2:26:55 - loss: 0.24 - ETA: 2:26:48 - loss: 0.24 - ETA: 2:26:43 - loss: 0.24 - ETA: 2:26:36 - loss: 0.24 - ETA: 2:26:30 - loss: 0.24 - ETA: 2:26:23 - loss: 0.24 - ETA: 2:26:17 - loss: 0.24 - ETA: 2:26:10 - loss: 0.23 - ETA: 2:26:04 - loss: 0.23 - ETA: 2:25:57 - loss: 0.23 - ETA: 2:25:52 - loss: 0.23 - ETA: 2:25:52 - loss: 0.23 - ETA: 2:25:52 - loss: 0.23 - ETA: 2:25:49 - loss: 0.23 - ETA: 2:25:42 - loss: 0.23 - ETA: 2:25:36 - loss: 0.23 - ETA: 2:25:29 - loss: 0.23 - ETA: 2:25:22 - loss: 0.23 - ETA: 2:25:16 - loss: 0.23 - ETA: 2:25:09 - loss: 0.23 - ETA: 2:25:03 - loss: 0.23 - ETA: 2:24:56 - loss: 0.23 - ETA: 2:24:50 - loss: 0.23 - ETA: 2:24:44 - loss: 0.23 - ETA: 2:24:38 - loss: 0.22 - ETA: 2:24:34 - loss: 0.22 - ETA: 2:24:27 - loss: 0.22 - ETA: 2:24:20 - loss: 0.22 - ETA: 2:24:14 - loss: 0.22 - ETA: 2:24:09 - loss: 0.22 - ETA: 2:24:06 - loss: 0.22 - ETA: 2:24:04 - loss: 0.22 - ETA: 2:24:01 - loss: 0.22 - ETA: 2:23:56 - loss: 0.22 - ETA: 2:23:49 - loss: 0.22 - ETA: 2:23:43 - loss: 0.22 - ETA: 2:23:36 - loss: 0.22 - ETA: 2:23:30 - loss: 0.22 - ETA: 2:23:24 - loss: 0.22 - ETA: 2:23:17 - loss: 0.22 - ETA: 2:23:11 - loss: 0.22 - ETA: 2:23:05 - loss: 0.22 - ETA: 2:22:59 - loss: 0.22 - ETA: 2:22:53 - loss: 0.22 - ETA: 2:22:47 - loss: 0.22 - ETA: 2:22:42 - loss: 0.22 - ETA: 2:22:35 - loss: 0.22 - ETA: 2:22:29 - loss: 0.21 - ETA: 2:22:24 - loss: 0.21 - ETA: 2:22:19 - loss: 0.21 - ETA: 2:22:12 - loss: 0.21 - ETA: 2:22:06 - loss: 0.21 - ETA: 2:22:01 - loss: 0.21 - ETA: 2:21:55 - loss: 0.21 - ETA: 2:21:50 - loss: 0.21 - ETA: 2:21:46 - loss: 0.21 - ETA: 2:21:40 - loss: 0.21 - ETA: 2:21:34 - loss: 0.21 - ETA: 2:21:28 - loss: 0.21 - ETA: 2:21:22 - loss: 0.21 - ETA: 2:21:17 - loss: 0.21 - ETA: 2:21:11 - loss: 0.21 - ETA: 2:21:06 - loss: 0.21 - ETA: 2:21:00 - loss: 0.21 - ETA: 2:20:54 - loss: 0.21 - ETA: 2:20:49 - loss: 0.21 - ETA: 2:20:44 - loss: 0.21 - ETA: 2:20:38 - loss: 0.21 - ETA: 2:20:33 - loss: 0.21 - ETA: 2:20:27 - loss: 0.21 - ETA: 2:20:22 - loss: 0.21 - ETA: 2:20:17 - loss: 0.21 - ETA: 2:20:11 - loss: 0.20 - ETA: 2:20:06 - loss: 0.20 - ETA: 2:20:00 - loss: 0.20 - ETA: 2:19:55 - loss: 0.20 - ETA: 2:19:51 - loss: 0.20 - ETA: 2:19:51 - loss: 0.20 - ETA: 2:19:46 - loss: 0.20 - ETA: 2:19:41 - loss: 0.20 - ETA: 2:19:35 - loss: 0.20 - ETA: 2:19:30 - loss: 0.20 - ETA: 2:19:25 - loss: 0.20 - ETA: 2:19:19 - loss: 0.20 - ETA: 2:19:14 - loss: 0.20 - ETA: 2:19:09 - loss: 0.20 - ETA: 2:19:04 - loss: 0.20 - ETA: 2:18:58 - loss: 0.20 - ETA: 2:18:53 - loss: 0.20 - ETA: 2:18:48 - loss: 0.20 - ETA: 2:18:42 - loss: 0.20 - ETA: 2:18:38 - loss: 0.20 - ETA: 2:18:33 - loss: 0.20 - ETA: 2:18:27 - loss: 0.20 - ETA: 2:18:21 - loss: 0.20 - ETA: 2:18:16 - loss: 0.20 - ETA: 2:18:11 - loss: 0.20 - ETA: 2:18:07 - loss: 0.2029\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 37120/154783 [======>.......................] - ETA: 2:18:01 - loss: 0.20 - ETA: 2:17:56 - loss: 0.20 - ETA: 2:17:51 - loss: 0.20 - ETA: 2:17:46 - loss: 0.20 - ETA: 2:17:41 - loss: 0.20 - ETA: 2:17:37 - loss: 0.20 - ETA: 2:17:33 - loss: 0.20 - ETA: 2:17:28 - loss: 0.20 - ETA: 2:17:23 - loss: 0.20 - ETA: 2:17:18 - loss: 0.19 - ETA: 2:17:13 - loss: 0.19 - ETA: 2:17:08 - loss: 0.19 - ETA: 2:17:04 - loss: 0.19 - ETA: 2:16:59 - loss: 0.19 - ETA: 2:16:54 - loss: 0.19 - ETA: 2:16:49 - loss: 0.19 - ETA: 2:16:46 - loss: 0.19 - ETA: 2:16:41 - loss: 0.19 - ETA: 2:16:37 - loss: 0.19 - ETA: 2:16:32 - loss: 0.19 - ETA: 2:16:28 - loss: 0.19 - ETA: 2:16:23 - loss: 0.19 - ETA: 2:16:18 - loss: 0.19 - ETA: 2:16:13 - loss: 0.19 - ETA: 2:16:09 - loss: 0.19 - ETA: 2:16:04 - loss: 0.19 - ETA: 2:16:00 - loss: 0.19 - ETA: 2:15:54 - loss: 0.19 - ETA: 2:15:49 - loss: 0.19 - ETA: 2:15:44 - loss: 0.19 - ETA: 2:15:39 - loss: 0.19 - ETA: 2:15:33 - loss: 0.19 - ETA: 2:15:29 - loss: 0.19 - ETA: 2:15:24 - loss: 0.19 - ETA: 2:15:18 - loss: 0.19 - ETA: 2:15:14 - loss: 0.19 - ETA: 2:15:09 - loss: 0.19 - ETA: 2:15:05 - loss: 0.19 - ETA: 2:15:01 - loss: 0.19 - ETA: 2:14:56 - loss: 0.19 - ETA: 2:14:51 - loss: 0.18 - ETA: 2:14:46 - loss: 0.18 - ETA: 2:14:41 - loss: 0.18 - ETA: 2:14:37 - loss: 0.18 - ETA: 2:14:32 - loss: 0.18 - ETA: 2:14:27 - loss: 0.18 - ETA: 2:14:23 - loss: 0.18 - ETA: 2:14:18 - loss: 0.18 - ETA: 2:14:14 - loss: 0.18 - ETA: 2:14:10 - loss: 0.18 - ETA: 2:14:10 - loss: 0.18 - ETA: 2:14:07 - loss: 0.18 - ETA: 2:14:03 - loss: 0.18 - ETA: 2:13:58 - loss: 0.18 - ETA: 2:13:54 - loss: 0.18 - ETA: 2:13:50 - loss: 0.18 - ETA: 2:13:46 - loss: 0.18 - ETA: 2:13:41 - loss: 0.18 - ETA: 2:13:37 - loss: 0.18 - ETA: 2:13:33 - loss: 0.18 - ETA: 2:13:29 - loss: 0.18 - ETA: 2:13:26 - loss: 0.18 - ETA: 2:13:22 - loss: 0.18 - ETA: 2:13:17 - loss: 0.18 - ETA: 2:13:13 - loss: 0.18 - ETA: 2:13:08 - loss: 0.18 - ETA: 2:13:04 - loss: 0.18 - ETA: 2:12:59 - loss: 0.18 - ETA: 2:12:54 - loss: 0.18 - ETA: 2:12:49 - loss: 0.18 - ETA: 2:12:45 - loss: 0.18 - ETA: 2:12:40 - loss: 0.18 - ETA: 2:12:36 - loss: 0.18 - ETA: 2:12:31 - loss: 0.18 - ETA: 2:12:26 - loss: 0.18 - ETA: 2:12:22 - loss: 0.18 - ETA: 2:12:18 - loss: 0.18 - ETA: 2:12:13 - loss: 0.18 - ETA: 2:12:09 - loss: 0.17 - ETA: 2:12:04 - loss: 0.17 - ETA: 2:12:00 - loss: 0.17 - ETA: 2:11:55 - loss: 0.17 - ETA: 2:11:50 - loss: 0.17 - ETA: 2:11:45 - loss: 0.17 - ETA: 2:11:41 - loss: 0.17 - ETA: 2:11:36 - loss: 0.17 - ETA: 2:11:32 - loss: 0.17 - ETA: 2:11:27 - loss: 0.17 - ETA: 2:11:23 - loss: 0.17 - ETA: 2:11:19 - loss: 0.17 - ETA: 2:11:15 - loss: 0.17 - ETA: 2:11:10 - loss: 0.17 - ETA: 2:11:06 - loss: 0.17 - ETA: 2:11:02 - loss: 0.17 - ETA: 2:10:57 - loss: 0.17 - ETA: 2:10:53 - loss: 0.17 - ETA: 2:10:48 - loss: 0.17 - ETA: 2:10:44 - loss: 0.17 - ETA: 2:10:39 - loss: 0.17 - ETA: 2:10:35 - loss: 0.17 - ETA: 2:10:31 - loss: 0.17 - ETA: 2:10:27 - loss: 0.17 - ETA: 2:10:24 - loss: 0.17 - ETA: 2:10:19 - loss: 0.17 - ETA: 2:10:15 - loss: 0.17 - ETA: 2:10:11 - loss: 0.17 - ETA: 2:10:06 - loss: 0.17 - ETA: 2:10:02 - loss: 0.17 - ETA: 2:09:57 - loss: 0.17 - ETA: 2:09:53 - loss: 0.17 - ETA: 2:09:48 - loss: 0.17 - ETA: 2:09:44 - loss: 0.17 - ETA: 2:09:39 - loss: 0.17 - ETA: 2:09:35 - loss: 0.17 - ETA: 2:09:31 - loss: 0.17 - ETA: 2:09:27 - loss: 0.17 - ETA: 2:09:22 - loss: 0.17 - ETA: 2:09:18 - loss: 0.17 - ETA: 2:09:15 - loss: 0.17 - ETA: 2:09:13 - loss: 0.17 - ETA: 2:09:11 - loss: 0.17 - ETA: 2:09:09 - loss: 0.17 - ETA: 2:09:07 - loss: 0.17 - ETA: 2:09:05 - loss: 0.16 - ETA: 2:09:02 - loss: 0.16 - ETA: 2:09:00 - loss: 0.16 - ETA: 2:08:59 - loss: 0.16 - ETA: 2:08:54 - loss: 0.16 - ETA: 2:08:50 - loss: 0.16 - ETA: 2:08:45 - loss: 0.16 - ETA: 2:08:40 - loss: 0.16 - ETA: 2:08:36 - loss: 0.16 - ETA: 2:08:32 - loss: 0.16 - ETA: 2:08:27 - loss: 0.16 - ETA: 2:08:23 - loss: 0.16 - ETA: 2:08:19 - loss: 0.16 - ETA: 2:08:15 - loss: 0.16 - ETA: 2:08:11 - loss: 0.16 - ETA: 2:08:07 - loss: 0.16 - ETA: 2:08:03 - loss: 0.16 - ETA: 2:07:58 - loss: 0.16 - ETA: 2:07:54 - loss: 0.16 - ETA: 2:07:50 - loss: 0.16 - ETA: 2:07:46 - loss: 0.16 - ETA: 2:07:42 - loss: 0.16 - ETA: 2:07:38 - loss: 0.16 - ETA: 2:07:34 - loss: 0.16 - ETA: 2:07:30 - loss: 0.16 - ETA: 2:07:25 - loss: 0.16 - ETA: 2:07:21 - loss: 0.16 - ETA: 2:07:18 - loss: 0.16 - ETA: 2:07:13 - loss: 0.16 - ETA: 2:07:09 - loss: 0.16 - ETA: 2:07:05 - loss: 0.16 - ETA: 2:07:01 - loss: 0.16 - ETA: 2:06:57 - loss: 0.16 - ETA: 2:06:53 - loss: 0.16 - ETA: 2:06:49 - loss: 0.16 - ETA: 2:06:45 - loss: 0.16 - ETA: 2:06:41 - loss: 0.16 - ETA: 2:06:37 - loss: 0.16 - ETA: 2:06:33 - loss: 0.16 - ETA: 2:06:29 - loss: 0.16 - ETA: 2:06:26 - loss: 0.16 - ETA: 2:06:22 - loss: 0.16 - ETA: 2:06:17 - loss: 0.16 - ETA: 2:06:13 - loss: 0.16 - ETA: 2:06:08 - loss: 0.16 - ETA: 2:06:04 - loss: 0.16 - ETA: 2:05:59 - loss: 0.16 - ETA: 2:05:56 - loss: 0.16 - ETA: 2:05:51 - loss: 0.16 - ETA: 2:05:47 - loss: 0.16 - ETA: 2:05:42 - loss: 0.16 - ETA: 2:05:38 - loss: 0.16 - ETA: 2:05:33 - loss: 0.16 - ETA: 2:05:29 - loss: 0.16 - ETA: 2:05:25 - loss: 0.16 - ETA: 2:05:22 - loss: 0.16 - ETA: 2:05:18 - loss: 0.16 - ETA: 2:05:14 - loss: 0.16 - ETA: 2:05:09 - loss: 0.16 - ETA: 2:05:05 - loss: 0.16 - ETA: 2:05:01 - loss: 0.16 - ETA: 2:04:57 - loss: 0.16 - ETA: 2:04:53 - loss: 0.16 - ETA: 2:04:49 - loss: 0.16 - ETA: 2:04:45 - loss: 0.16 - ETA: 2:04:40 - loss: 0.15 - ETA: 2:04:36 - loss: 0.15 - ETA: 2:04:32 - loss: 0.15 - ETA: 2:04:28 - loss: 0.15 - ETA: 2:04:24 - loss: 0.15 - ETA: 2:04:26 - loss: 0.15 - ETA: 2:04:31 - loss: 0.15 - ETA: 2:04:33 - loss: 0.15 - ETA: 2:04:36 - loss: 0.15 - ETA: 2:04:36 - loss: 0.15 - ETA: 2:04:40 - loss: 0.15 - ETA: 2:04:40 - loss: 0.15 - ETA: 2:04:40 - loss: 0.15 - ETA: 2:04:40 - loss: 0.15 - ETA: 2:04:41 - loss: 0.15 - ETA: 2:04:42 - loss: 0.15 - ETA: 2:04:43 - loss: 0.15 - ETA: 2:04:43 - loss: 0.15 - ETA: 2:04:42 - loss: 0.15 - ETA: 2:04:39 - loss: 0.15 - ETA: 2:04:34 - loss: 0.15 - ETA: 2:04:30 - loss: 0.15 - ETA: 2:04:28 - loss: 0.15 - ETA: 2:04:26 - loss: 0.15 - ETA: 2:04:25 - loss: 0.15 - ETA: 2:04:23 - loss: 0.15 - ETA: 2:04:22 - loss: 0.15 - ETA: 2:04:20 - loss: 0.15 - ETA: 2:04:19 - loss: 0.15 - ETA: 2:04:17 - loss: 0.15 - ETA: 2:04:15 - loss: 0.15 - ETA: 2:04:13 - loss: 0.15 - ETA: 2:04:11 - loss: 0.15 - ETA: 2:04:10 - loss: 0.15 - ETA: 2:04:08 - loss: 0.15 - ETA: 2:04:05 - loss: 0.15 - ETA: 2:04:06 - loss: 0.15 - ETA: 2:04:04 - loss: 0.15 - ETA: 2:04:02 - loss: 0.15 - ETA: 2:04:00 - loss: 0.15 - ETA: 2:03:57 - loss: 0.15 - ETA: 2:03:55 - loss: 0.15 - ETA: 4:31:40 - loss: 0.15 - ETA: 4:31:28 - loss: 0.15 - ETA: 4:31:13 - loss: 0.15 - ETA: 4:30:51 - loss: 0.15 - ETA: 4:30:31 - loss: 0.15 - ETA: 4:30:13 - loss: 0.15 - ETA: 4:29:55 - loss: 0.15 - ETA: 4:29:35 - loss: 0.15 - ETA: 4:29:16 - loss: 0.15 - ETA: 4:28:54 - loss: 0.15 - ETA: 4:28:29 - loss: 0.15 - ETA: 4:28:07 - loss: 0.15 - ETA: 4:27:43 - loss: 0.15 - ETA: 4:27:20 - loss: 0.15 - ETA: 4:26:57 - loss: 0.15 - ETA: 4:26:34 - loss: 0.15 - ETA: 4:26:11 - loss: 0.15 - ETA: 4:25:48 - loss: 0.15 - ETA: 4:25:25 - loss: 0.15 - ETA: 4:25:02 - loss: 0.15 - ETA: 4:24:40 - loss: 0.15 - ETA: 4:24:17 - loss: 0.15 - ETA: 4:23:54 - loss: 0.15 - ETA: 4:23:31 - loss: 0.15 - ETA: 4:23:14 - loss: 0.15 - ETA: 4:22:59 - loss: 0.15 - ETA: 4:22:46 - loss: 0.15 - ETA: 4:22:28 - loss: 0.14 - ETA: 4:22:06 - loss: 0.14 - ETA: 4:21:44 - loss: 0.14 - ETA: 4:21:22 - loss: 0.14 - ETA: 4:21:02 - loss: 0.14 - ETA: 4:20:41 - loss: 0.14 - ETA: 4:20:19 - loss: 0.14 - ETA: 4:19:57 - loss: 0.14 - ETA: 4:19:35 - loss: 0.14 - ETA: 4:19:13 - loss: 0.14 - ETA: 4:18:52 - loss: 0.14 - ETA: 4:18:31 - loss: 0.14 - ETA: 4:18:09 - loss: 0.14 - ETA: 4:17:47 - loss: 0.14 - ETA: 4:17:26 - loss: 0.14 - ETA: 4:17:05 - loss: 0.14 - ETA: 4:16:43 - loss: 0.14 - ETA: 4:16:22 - loss: 0.14 - ETA: 4:16:01 - loss: 0.14 - ETA: 4:15:40 - loss: 0.14 - ETA: 4:15:20 - loss: 0.14 - ETA: 4:14:59 - loss: 0.14 - ETA: 4:14:41 - loss: 0.14 - ETA: 4:14:20 - loss: 0.14 - ETA: 4:14:08 - loss: 0.14 - ETA: 4:13:55 - loss: 0.14 - ETA: 4:13:35 - loss: 0.14 - ETA: 4:13:15 - loss: 0.14 - ETA: 4:12:55 - loss: 0.14 - ETA: 4:12:35 - loss: 0.14 - ETA: 4:12:15 - loss: 0.14 - ETA: 4:11:55 - loss: 0.14 - ETA: 4:11:36 - loss: 0.14 - ETA: 4:11:18 - loss: 0.1465 55744/154783 [=========>....................] - ETA: 4:11:02 - loss: 0.14 - ETA: 4:10:51 - loss: 0.14 - ETA: 4:10:32 - loss: 0.14 - ETA: 4:10:14 - loss: 0.14 - ETA: 4:09:57 - loss: 0.14 - ETA: 4:09:38 - loss: 0.14 - ETA: 4:09:18 - loss: 0.14 - ETA: 4:08:58 - loss: 0.14 - ETA: 4:08:39 - loss: 0.14 - ETA: 4:08:19 - loss: 0.14 - ETA: 4:07:59 - loss: 0.14 - ETA: 4:07:40 - loss: 0.14 - ETA: 4:07:20 - loss: 0.14 - ETA: 4:07:01 - loss: 0.14 - ETA: 4:06:41 - loss: 0.14 - ETA: 4:06:22 - loss: 0.14 - ETA: 4:06:07 - loss: 0.14 - ETA: 4:05:49 - loss: 0.14 - ETA: 4:05:31 - loss: 0.14 - ETA: 4:05:11 - loss: 0.14 - ETA: 4:04:52 - loss: 0.14 - ETA: 4:04:32 - loss: 0.14 - ETA: 4:04:13 - loss: 0.14 - ETA: 4:03:54 - loss: 0.14 - ETA: 4:03:34 - loss: 0.14 - ETA: 4:03:15 - loss: 0.14 - ETA: 4:02:56 - loss: 0.14 - ETA: 4:02:37 - loss: 0.14 - ETA: 4:02:19 - loss: 0.14 - ETA: 4:02:00 - loss: 0.14 - ETA: 4:01:41 - loss: 0.14 - ETA: 4:01:23 - loss: 0.14 - ETA: 4:01:06 - loss: 0.14 - ETA: 4:00:48 - loss: 0.14 - ETA: 4:00:30 - loss: 0.14 - ETA: 4:00:11 - loss: 0.14 - ETA: 3:59:53 - loss: 0.14 - ETA: 3:59:34 - loss: 0.14 - ETA: 3:59:16 - loss: 0.14 - ETA: 3:58:57 - loss: 0.14 - ETA: 3:58:39 - loss: 0.14 - ETA: 3:58:21 - loss: 0.14 - ETA: 3:58:03 - loss: 0.14 - ETA: 3:57:45 - loss: 0.14 - ETA: 3:57:27 - loss: 0.14 - ETA: 3:57:08 - loss: 0.14 - ETA: 3:56:50 - loss: 0.14 - ETA: 3:56:32 - loss: 0.14 - ETA: 3:56:14 - loss: 0.14 - ETA: 3:55:56 - loss: 0.14 - ETA: 3:55:38 - loss: 0.14 - ETA: 3:55:20 - loss: 0.14 - ETA: 3:55:02 - loss: 0.14 - ETA: 3:54:44 - loss: 0.14 - ETA: 3:54:26 - loss: 0.14 - ETA: 3:54:08 - loss: 0.14 - ETA: 3:53:50 - loss: 0.14 - ETA: 3:53:33 - loss: 0.14 - ETA: 3:53:15 - loss: 0.14 - ETA: 3:52:57 - loss: 0.14 - ETA: 3:52:40 - loss: 0.14 - ETA: 3:52:22 - loss: 0.14 - ETA: 3:52:05 - loss: 0.14 - ETA: 3:51:47 - loss: 0.13 - ETA: 3:51:30 - loss: 0.13 - ETA: 3:51:14 - loss: 0.13 - ETA: 3:51:06 - loss: 0.13 - ETA: 3:51:00 - loss: 0.13 - ETA: 3:50:48 - loss: 0.13 - ETA: 3:50:31 - loss: 0.13 - ETA: 3:50:14 - loss: 0.13 - ETA: 3:49:57 - loss: 0.13 - ETA: 3:49:40 - loss: 0.13 - ETA: 3:49:25 - loss: 0.13 - ETA: 3:49:09 - loss: 0.13 - ETA: 3:48:54 - loss: 0.13 - ETA: 3:48:37 - loss: 0.13 - ETA: 3:48:23 - loss: 0.13 - ETA: 3:48:09 - loss: 0.13 - ETA: 3:47:55 - loss: 0.13 - ETA: 3:47:39 - loss: 0.13 - ETA: 3:47:22 - loss: 0.13 - ETA: 3:47:08 - loss: 0.13 - ETA: 3:46:52 - loss: 0.13 - ETA: 3:46:42 - loss: 0.13 - ETA: 3:46:33 - loss: 0.13 - ETA: 3:46:23 - loss: 0.13 - ETA: 3:46:13 - loss: 0.13 - ETA: 3:46:05 - loss: 0.13 - ETA: 3:45:57 - loss: 0.13 - ETA: 3:45:41 - loss: 0.13 - ETA: 3:45:25 - loss: 0.13 - ETA: 3:45:09 - loss: 0.13 - ETA: 3:44:53 - loss: 0.13 - ETA: 3:44:37 - loss: 0.13 - ETA: 3:44:23 - loss: 0.13 - ETA: 3:44:08 - loss: 0.13 - ETA: 3:43:53 - loss: 0.13 - ETA: 3:43:38 - loss: 0.13 - ETA: 3:43:23 - loss: 0.13 - ETA: 3:43:08 - loss: 0.13 - ETA: 3:42:53 - loss: 0.13 - ETA: 3:42:38 - loss: 0.13 - ETA: 3:42:23 - loss: 0.13 - ETA: 3:42:10 - loss: 0.13 - ETA: 3:41:56 - loss: 0.13 - ETA: 3:41:41 - loss: 0.13 - ETA: 3:41:26 - loss: 0.13 - ETA: 3:41:11 - loss: 0.13 - ETA: 3:40:55 - loss: 0.13 - ETA: 3:40:40 - loss: 0.13 - ETA: 3:40:25 - loss: 0.13 - ETA: 3:40:10 - loss: 0.13 - ETA: 3:39:56 - loss: 0.13 - ETA: 3:39:48 - loss: 0.13 - ETA: 3:39:37 - loss: 0.13 - ETA: 3:39:24 - loss: 0.13 - ETA: 3:39:08 - loss: 0.13 - ETA: 3:38:56 - loss: 0.13 - ETA: 3:38:42 - loss: 0.13 - ETA: 3:38:29 - loss: 0.13 - ETA: 3:38:19 - loss: 0.13 - ETA: 3:38:06 - loss: 0.13 - ETA: 3:37:54 - loss: 0.13 - ETA: 3:37:40 - loss: 0.13 - ETA: 3:37:25 - loss: 0.13 - ETA: 3:37:12 - loss: 0.13 - ETA: 3:36:58 - loss: 0.13 - ETA: 3:36:44 - loss: 0.13 - ETA: 3:36:31 - loss: 0.13 - ETA: 3:36:16 - loss: 0.13 - ETA: 3:36:04 - loss: 0.13 - ETA: 3:35:50 - loss: 0.13 - ETA: 3:35:36 - loss: 0.13 - ETA: 3:35:25 - loss: 0.13 - ETA: 3:35:16 - loss: 0.13 - ETA: 3:35:03 - loss: 0.13 - ETA: 3:34:50 - loss: 0.13 - ETA: 3:34:35 - loss: 0.13 - ETA: 3:34:21 - loss: 0.13 - ETA: 3:34:07 - loss: 0.13 - ETA: 3:33:54 - loss: 0.13 - ETA: 3:33:39 - loss: 0.13 - ETA: 3:33:25 - loss: 0.13 - ETA: 3:33:11 - loss: 0.13 - ETA: 3:32:56 - loss: 0.13 - ETA: 3:32:41 - loss: 0.13 - ETA: 3:32:28 - loss: 0.13 - ETA: 3:32:14 - loss: 0.13 - ETA: 3:32:01 - loss: 0.13 - ETA: 3:31:48 - loss: 0.13 - ETA: 3:31:35 - loss: 0.13 - ETA: 3:31:22 - loss: 0.13 - ETA: 3:31:10 - loss: 0.13 - ETA: 3:30:58 - loss: 0.13 - ETA: 3:30:44 - loss: 0.13 - ETA: 3:30:30 - loss: 0.13 - ETA: 3:30:15 - loss: 0.13 - ETA: 3:30:01 - loss: 0.13 - ETA: 3:29:47 - loss: 0.13 - ETA: 3:29:33 - loss: 0.13 - ETA: 3:29:20 - loss: 0.13 - ETA: 3:29:05 - loss: 0.13 - ETA: 3:28:51 - loss: 0.13 - ETA: 3:28:37 - loss: 0.13 - ETA: 3:28:23 - loss: 0.13 - ETA: 3:28:09 - loss: 0.13 - ETA: 3:27:55 - loss: 0.13 - ETA: 3:27:41 - loss: 0.13 - ETA: 3:27:27 - loss: 0.13 - ETA: 3:27:13 - loss: 0.13 - ETA: 3:26:59 - loss: 0.13 - ETA: 3:26:46 - loss: 0.13 - ETA: 3:26:32 - loss: 0.13 - ETA: 3:26:18 - loss: 0.13 - ETA: 3:26:04 - loss: 0.13 - ETA: 3:25:51 - loss: 0.13 - ETA: 3:25:37 - loss: 0.13 - ETA: 3:25:23 - loss: 0.13 - ETA: 3:25:10 - loss: 0.13 - ETA: 3:24:57 - loss: 0.13 - ETA: 3:24:43 - loss: 0.13 - ETA: 3:24:30 - loss: 0.13 - ETA: 3:24:18 - loss: 0.13 - ETA: 3:24:05 - loss: 0.12 - ETA: 3:23:52 - loss: 0.12 - ETA: 3:23:39 - loss: 0.12 - ETA: 3:23:27 - loss: 0.12 - ETA: 3:23:13 - loss: 0.12 - ETA: 3:23:01 - loss: 0.12 - ETA: 3:22:51 - loss: 0.12 - ETA: 3:22:39 - loss: 0.12 - ETA: 3:22:29 - loss: 0.12 - ETA: 3:22:24 - loss: 0.12 - ETA: 3:22:14 - loss: 0.12 - ETA: 3:22:03 - loss: 0.12 - ETA: 3:21:51 - loss: 0.12 - ETA: 3:21:38 - loss: 0.12 - ETA: 3:21:26 - loss: 0.12 - ETA: 3:21:14 - loss: 0.12 - ETA: 3:21:01 - loss: 0.12 - ETA: 3:20:50 - loss: 0.12 - ETA: 3:20:37 - loss: 0.12 - ETA: 3:20:24 - loss: 0.12 - ETA: 3:20:16 - loss: 0.12 - ETA: 3:20:04 - loss: 0.12 - ETA: 3:19:51 - loss: 0.12 - ETA: 3:19:39 - loss: 0.12 - ETA: 3:19:26 - loss: 0.12 - ETA: 3:19:14 - loss: 0.12 - ETA: 3:19:01 - loss: 0.12 - ETA: 3:18:48 - loss: 0.12 - ETA: 3:18:34 - loss: 0.12 - ETA: 3:18:21 - loss: 0.12 - ETA: 3:18:07 - loss: 0.12 - ETA: 3:17:54 - loss: 0.12 - ETA: 3:17:43 - loss: 0.12 - ETA: 3:17:31 - loss: 0.12 - ETA: 3:17:18 - loss: 0.12 - ETA: 3:17:05 - loss: 0.12 - ETA: 3:16:53 - loss: 0.12 - ETA: 3:16:40 - loss: 0.12 - ETA: 3:16:27 - loss: 0.12 - ETA: 3:16:17 - loss: 0.12 - ETA: 3:16:07 - loss: 0.12 - ETA: 3:15:57 - loss: 0.12 - ETA: 3:15:44 - loss: 0.12 - ETA: 3:15:31 - loss: 0.12 - ETA: 3:15:17 - loss: 0.12 - ETA: 3:15:03 - loss: 0.12 - ETA: 3:14:53 - loss: 0.12 - ETA: 3:14:43 - loss: 0.12 - ETA: 3:14:34 - loss: 0.12 - ETA: 3:14:27 - loss: 0.12 - ETA: 3:14:20 - loss: 0.12 - ETA: 3:14:15 - loss: 0.12 - ETA: 3:14:05 - loss: 0.12 - ETA: 3:13:57 - loss: 0.12 - ETA: 3:13:50 - loss: 0.12 - ETA: 3:13:41 - loss: 0.12 - ETA: 3:13:32 - loss: 0.12 - ETA: 3:13:22 - loss: 0.12 - ETA: 3:13:13 - loss: 0.12 - ETA: 3:13:03 - loss: 0.12 - ETA: 3:12:51 - loss: 0.12 - ETA: 3:12:40 - loss: 0.12 - ETA: 3:12:32 - loss: 0.12 - ETA: 3:12:21 - loss: 0.12 - ETA: 3:12:12 - loss: 0.12 - ETA: 3:12:00 - loss: 0.12 - ETA: 3:11:49 - loss: 0.12 - ETA: 3:11:37 - loss: 0.12 - ETA: 3:11:28 - loss: 0.12 - ETA: 3:11:19 - loss: 0.12 - ETA: 3:11:10 - loss: 0.12 - ETA: 3:10:58 - loss: 0.12 - ETA: 3:10:45 - loss: 0.12 - ETA: 3:10:32 - loss: 0.12 - ETA: 3:10:19 - loss: 0.12 - ETA: 3:10:05 - loss: 0.12 - ETA: 3:10:06 - loss: 0.12 - ETA: 3:09:59 - loss: 0.12 - ETA: 3:09:56 - loss: 0.12 - ETA: 3:09:49 - loss: 0.12 - ETA: 3:09:42 - loss: 0.12 - ETA: 3:09:35 - loss: 0.12 - ETA: 3:09:29 - loss: 0.12 - ETA: 3:09:23 - loss: 0.12 - ETA: 3:09:12 - loss: 0.12 - ETA: 3:09:00 - loss: 0.12 - ETA: 3:08:48 - loss: 0.12 - ETA: 3:08:35 - loss: 0.12 - ETA: 3:08:23 - loss: 0.12 - ETA: 3:08:10 - loss: 0.12 - ETA: 3:07:57 - loss: 0.12 - ETA: 3:07:44 - loss: 0.12 - ETA: 3:07:34 - loss: 0.12 - ETA: 3:07:29 - loss: 0.12 - ETA: 3:07:24 - loss: 0.12 - ETA: 3:07:21 - loss: 0.12 - ETA: 3:07:14 - loss: 0.12 - ETA: 3:07:04 - loss: 0.12 - ETA: 3:06:52 - loss: 0.12 - ETA: 3:06:41 - loss: 0.12 - ETA: 3:06:32 - loss: 0.12 - ETA: 3:06:21 - loss: 0.12 - ETA: 3:06:11 - loss: 0.12 - ETA: 3:06:00 - loss: 0.12 - ETA: 3:05:48 - loss: 0.12 - ETA: 3:05:38 - loss: 0.12 - ETA: 3:05:25 - loss: 0.1233 74368/154783 [=============>................] - ETA: 3:05:13 - loss: 0.12 - ETA: 3:05:00 - loss: 0.12 - ETA: 3:04:48 - loss: 0.12 - ETA: 3:04:36 - loss: 0.12 - ETA: 3:04:24 - loss: 0.12 - ETA: 3:04:11 - loss: 0.12 - ETA: 3:03:58 - loss: 0.12 - ETA: 3:03:45 - loss: 0.12 - ETA: 3:03:32 - loss: 0.12 - ETA: 3:03:19 - loss: 0.12 - ETA: 3:03:05 - loss: 0.12 - ETA: 3:02:52 - loss: 0.12 - ETA: 3:02:39 - loss: 0.12 - ETA: 3:02:26 - loss: 0.12 - ETA: 3:02:13 - loss: 0.12 - ETA: 3:02:00 - loss: 0.12 - ETA: 3:01:47 - loss: 0.12 - ETA: 3:01:35 - loss: 0.12 - ETA: 3:01:23 - loss: 0.12 - ETA: 3:01:11 - loss: 0.12 - ETA: 3:00:58 - loss: 0.12 - ETA: 3:00:45 - loss: 0.12 - ETA: 3:00:33 - loss: 0.12 - ETA: 3:00:21 - loss: 0.12 - ETA: 3:00:09 - loss: 0.12 - ETA: 2:59:56 - loss: 0.12 - ETA: 2:59:43 - loss: 0.12 - ETA: 2:59:31 - loss: 0.12 - ETA: 2:59:19 - loss: 0.12 - ETA: 2:59:06 - loss: 0.12 - ETA: 2:58:54 - loss: 0.12 - ETA: 2:58:42 - loss: 0.12 - ETA: 2:58:32 - loss: 0.12 - ETA: 2:58:22 - loss: 0.12 - ETA: 2:58:11 - loss: 0.12 - ETA: 2:57:59 - loss: 0.12 - ETA: 2:57:47 - loss: 0.12 - ETA: 2:57:35 - loss: 0.12 - ETA: 2:57:22 - loss: 0.12 - ETA: 2:57:10 - loss: 0.12 - ETA: 2:56:58 - loss: 0.12 - ETA: 2:56:46 - loss: 0.12 - ETA: 2:56:34 - loss: 0.12 - ETA: 2:56:23 - loss: 0.12 - ETA: 2:56:11 - loss: 0.12 - ETA: 2:55:58 - loss: 0.12 - ETA: 2:55:47 - loss: 0.12 - ETA: 2:55:36 - loss: 0.12 - ETA: 2:55:25 - loss: 0.12 - ETA: 2:55:14 - loss: 0.12 - ETA: 2:55:02 - loss: 0.12 - ETA: 2:54:52 - loss: 0.12 - ETA: 2:54:40 - loss: 0.12 - ETA: 2:54:28 - loss: 0.12 - ETA: 2:54:17 - loss: 0.12 - ETA: 2:54:05 - loss: 0.12 - ETA: 2:53:54 - loss: 0.12 - ETA: 2:53:42 - loss: 0.12 - ETA: 2:53:31 - loss: 0.12 - ETA: 2:53:21 - loss: 0.12 - ETA: 2:53:09 - loss: 0.12 - ETA: 2:53:01 - loss: 0.12 - ETA: 2:52:50 - loss: 0.12 - ETA: 2:52:40 - loss: 0.12 - ETA: 2:52:30 - loss: 0.12 - ETA: 2:52:21 - loss: 0.12 - ETA: 2:52:09 - loss: 0.12 - ETA: 2:51:58 - loss: 0.12 - ETA: 2:51:47 - loss: 0.12 - ETA: 2:51:36 - loss: 0.11 - ETA: 2:51:25 - loss: 0.11 - ETA: 2:51:15 - loss: 0.11 - ETA: 2:51:04 - loss: 0.11 - ETA: 2:50:53 - loss: 0.11 - ETA: 2:50:42 - loss: 0.11 - ETA: 2:50:30 - loss: 0.11 - ETA: 2:50:19 - loss: 0.11 - ETA: 2:50:08 - loss: 0.11 - ETA: 2:49:57 - loss: 0.11 - ETA: 2:49:46 - loss: 0.11 - ETA: 2:49:34 - loss: 0.11 - ETA: 2:49:23 - loss: 0.11 - ETA: 2:49:13 - loss: 0.11 - ETA: 2:49:02 - loss: 0.11 - ETA: 2:48:51 - loss: 0.11 - ETA: 2:48:40 - loss: 0.11 - ETA: 2:48:32 - loss: 0.11 - ETA: 2:48:22 - loss: 0.11 - ETA: 2:48:12 - loss: 0.11 - ETA: 2:48:01 - loss: 0.11 - ETA: 2:47:51 - loss: 0.11 - ETA: 2:47:41 - loss: 0.11 - ETA: 2:47:30 - loss: 0.11 - ETA: 2:47:19 - loss: 0.11 - ETA: 2:47:09 - loss: 0.11 - ETA: 2:47:00 - loss: 0.11 - ETA: 2:46:52 - loss: 0.11 - ETA: 2:46:42 - loss: 0.11 - ETA: 2:46:33 - loss: 0.11 - ETA: 2:46:24 - loss: 0.11 - ETA: 2:46:14 - loss: 0.11 - ETA: 2:46:04 - loss: 0.11 - ETA: 2:45:55 - loss: 0.11 - ETA: 2:45:46 - loss: 0.11 - ETA: 2:45:35 - loss: 0.11 - ETA: 2:45:26 - loss: 0.11 - ETA: 2:45:16 - loss: 0.11 - ETA: 2:45:08 - loss: 0.11 - ETA: 2:44:59 - loss: 0.11 - ETA: 2:44:48 - loss: 0.11 - ETA: 2:44:37 - loss: 0.11 - ETA: 2:44:28 - loss: 0.11 - ETA: 2:44:18 - loss: 0.11 - ETA: 2:44:08 - loss: 0.11 - ETA: 2:43:58 - loss: 0.11 - ETA: 2:43:46 - loss: 0.11 - ETA: 2:43:35 - loss: 0.11 - ETA: 2:43:24 - loss: 0.11 - ETA: 2:43:15 - loss: 0.11 - ETA: 2:43:05 - loss: 0.11 - ETA: 2:42:55 - loss: 0.11 - ETA: 2:42:43 - loss: 0.11 - ETA: 2:42:32 - loss: 0.11 - ETA: 2:42:22 - loss: 0.11 - ETA: 2:42:11 - loss: 0.11 - ETA: 2:42:00 - loss: 0.11 - ETA: 2:41:50 - loss: 0.11 - ETA: 2:41:40 - loss: 0.11 - ETA: 2:41:30 - loss: 0.11 - ETA: 2:41:20 - loss: 0.11 - ETA: 2:41:11 - loss: 0.11 - ETA: 2:41:01 - loss: 0.11 - ETA: 2:40:50 - loss: 0.11 - ETA: 2:40:39 - loss: 0.11 - ETA: 2:40:29 - loss: 0.11 - ETA: 2:40:20 - loss: 0.11 - ETA: 2:40:09 - loss: 0.11 - ETA: 2:39:58 - loss: 0.11 - ETA: 2:39:48 - loss: 0.11 - ETA: 2:39:37 - loss: 0.11 - ETA: 2:39:26 - loss: 0.11 - ETA: 2:39:15 - loss: 0.11 - ETA: 2:39:05 - loss: 0.11 - ETA: 2:38:54 - loss: 0.11 - ETA: 2:38:43 - loss: 0.11 - ETA: 2:38:32 - loss: 0.11 - ETA: 2:38:21 - loss: 0.11 - ETA: 2:38:11 - loss: 0.11 - ETA: 2:38:00 - loss: 0.11 - ETA: 2:37:49 - loss: 0.11 - ETA: 2:37:38 - loss: 0.11 - ETA: 2:37:27 - loss: 0.11 - ETA: 2:37:16 - loss: 0.11 - ETA: 2:37:05 - loss: 0.11 - ETA: 2:36:54 - loss: 0.11 - ETA: 2:36:44 - loss: 0.11 - ETA: 2:36:35 - loss: 0.11 - ETA: 2:36:25 - loss: 0.11 - ETA: 2:36:16 - loss: 0.11 - ETA: 2:36:07 - loss: 0.11 - ETA: 2:35:58 - loss: 0.11 - ETA: 2:35:49 - loss: 0.11 - ETA: 2:35:40 - loss: 0.11 - ETA: 2:35:29 - loss: 0.11 - ETA: 2:35:18 - loss: 0.11 - ETA: 2:35:07 - loss: 0.11 - ETA: 2:34:57 - loss: 0.11 - ETA: 2:34:46 - loss: 0.11 - ETA: 2:34:35 - loss: 0.11 - ETA: 2:34:24 - loss: 0.11 - ETA: 2:34:14 - loss: 0.11 - ETA: 2:34:03 - loss: 0.11 - ETA: 2:33:52 - loss: 0.11 - ETA: 2:33:41 - loss: 0.11 - ETA: 2:33:31 - loss: 0.11 - ETA: 2:33:20 - loss: 0.11 - ETA: 2:33:10 - loss: 0.11 - ETA: 2:33:01 - loss: 0.11 - ETA: 2:32:54 - loss: 0.11 - ETA: 2:32:44 - loss: 0.11 - ETA: 2:32:33 - loss: 0.11 - ETA: 2:32:23 - loss: 0.11 - ETA: 2:32:13 - loss: 0.11 - ETA: 2:32:04 - loss: 0.11 - ETA: 2:31:54 - loss: 0.11 - ETA: 2:31:44 - loss: 0.11 - ETA: 2:31:34 - loss: 0.11 - ETA: 2:31:25 - loss: 0.11 - ETA: 2:31:15 - loss: 0.11 - ETA: 2:31:04 - loss: 0.11 - ETA: 2:30:55 - loss: 0.11 - ETA: 2:30:46 - loss: 0.11 - ETA: 2:30:36 - loss: 0.11 - ETA: 2:30:26 - loss: 0.11 - ETA: 2:30:15 - loss: 0.11 - ETA: 2:30:06 - loss: 0.11 - ETA: 2:29:57 - loss: 0.11 - ETA: 2:29:49 - loss: 0.11 - ETA: 2:29:41 - loss: 0.11 - ETA: 2:29:34 - loss: 0.11 - ETA: 2:29:24 - loss: 0.11 - ETA: 2:29:16 - loss: 0.11 - ETA: 2:29:06 - loss: 0.11 - ETA: 2:28:57 - loss: 0.11 - ETA: 2:28:48 - loss: 0.11 - ETA: 2:28:39 - loss: 0.11 - ETA: 2:28:30 - loss: 0.11 - ETA: 2:28:22 - loss: 0.11 - ETA: 2:28:13 - loss: 0.11 - ETA: 2:28:03 - loss: 0.11 - ETA: 2:27:54 - loss: 0.11 - ETA: 2:27:44 - loss: 0.11 - ETA: 2:27:34 - loss: 0.11 - ETA: 2:27:23 - loss: 0.11 - ETA: 2:27:14 - loss: 0.11 - ETA: 2:27:05 - loss: 0.11 - ETA: 2:26:57 - loss: 0.11 - ETA: 2:26:50 - loss: 0.11 - ETA: 2:26:43 - loss: 0.11 - ETA: 2:26:34 - loss: 0.11 - ETA: 2:26:26 - loss: 0.11 - ETA: 2:26:18 - loss: 0.11 - ETA: 2:26:09 - loss: 0.11 - ETA: 2:26:01 - loss: 0.11 - ETA: 2:25:54 - loss: 0.11 - ETA: 2:25:48 - loss: 0.11 - ETA: 2:25:41 - loss: 0.11 - ETA: 2:25:32 - loss: 0.11 - ETA: 2:25:23 - loss: 0.11 - ETA: 2:25:13 - loss: 0.11 - ETA: 2:25:03 - loss: 0.11 - ETA: 2:24:54 - loss: 0.11 - ETA: 2:24:44 - loss: 0.11 - ETA: 2:24:35 - loss: 0.11 - ETA: 2:24:25 - loss: 0.11 - ETA: 2:24:15 - loss: 0.11 - ETA: 2:24:06 - loss: 0.11 - ETA: 2:23:57 - loss: 0.11 - ETA: 2:23:47 - loss: 0.11 - ETA: 2:23:38 - loss: 0.11 - ETA: 2:23:28 - loss: 0.11 - ETA: 2:23:18 - loss: 0.11 - ETA: 2:23:08 - loss: 0.11 - ETA: 2:22:58 - loss: 0.11 - ETA: 2:22:49 - loss: 0.11 - ETA: 2:22:39 - loss: 0.11 - ETA: 2:22:29 - loss: 0.11 - ETA: 2:22:19 - loss: 0.11 - ETA: 2:22:10 - loss: 0.11 - ETA: 2:22:00 - loss: 0.11 - ETA: 2:21:51 - loss: 0.11 - ETA: 2:21:42 - loss: 0.11 - ETA: 2:21:32 - loss: 0.11 - ETA: 2:21:23 - loss: 0.11 - ETA: 2:21:13 - loss: 0.11 - ETA: 2:21:03 - loss: 0.11 - ETA: 2:20:54 - loss: 0.11 - ETA: 2:20:44 - loss: 0.11 - ETA: 2:20:35 - loss: 0.11 - ETA: 2:20:25 - loss: 0.11 - ETA: 2:20:16 - loss: 0.11 - ETA: 2:20:06 - loss: 0.11 - ETA: 2:19:57 - loss: 0.11 - ETA: 2:19:48 - loss: 0.11 - ETA: 2:19:39 - loss: 0.11 - ETA: 2:19:29 - loss: 0.11 - ETA: 2:19:20 - loss: 0.11 - ETA: 2:19:11 - loss: 0.11 - ETA: 2:19:02 - loss: 0.11 - ETA: 2:18:52 - loss: 0.11 - ETA: 2:18:43 - loss: 0.11 - ETA: 2:18:34 - loss: 0.11 - ETA: 2:18:24 - loss: 0.11 - ETA: 2:18:15 - loss: 0.11 - ETA: 2:18:05 - loss: 0.11 - ETA: 2:17:56 - loss: 0.11 - ETA: 2:17:46 - loss: 0.11 - ETA: 2:17:37 - loss: 0.11 - ETA: 2:17:28 - loss: 0.11 - ETA: 2:17:19 - loss: 0.11 - ETA: 2:17:10 - loss: 0.11 - ETA: 2:17:00 - loss: 0.11 - ETA: 2:16:51 - loss: 0.11 - ETA: 2:16:42 - loss: 0.11 - ETA: 2:16:32 - loss: 0.11 - ETA: 2:16:23 - loss: 0.11 - ETA: 2:16:14 - loss: 0.11 - ETA: 2:16:05 - loss: 0.11 - ETA: 2:15:56 - loss: 0.11 - ETA: 2:15:47 - loss: 0.11 - ETA: 2:15:38 - loss: 0.1108 92992/154783 [=================>............] - ETA: 2:15:29 - loss: 0.11 - ETA: 2:15:20 - loss: 0.11 - ETA: 2:15:12 - loss: 0.11 - ETA: 2:15:03 - loss: 0.11 - ETA: 2:14:54 - loss: 0.11 - ETA: 2:14:45 - loss: 0.11 - ETA: 2:14:37 - loss: 0.11 - ETA: 2:14:28 - loss: 0.11 - ETA: 2:14:19 - loss: 0.11 - ETA: 2:14:10 - loss: 0.11 - ETA: 2:14:01 - loss: 0.11 - ETA: 2:13:52 - loss: 0.11 - ETA: 2:13:43 - loss: 0.11 - ETA: 2:13:34 - loss: 0.11 - ETA: 2:13:25 - loss: 0.11 - ETA: 2:13:16 - loss: 0.11 - ETA: 2:13:07 - loss: 0.11 - ETA: 2:12:58 - loss: 0.11 - ETA: 2:12:49 - loss: 0.11 - ETA: 2:12:40 - loss: 0.11 - ETA: 2:12:31 - loss: 0.11 - ETA: 2:12:22 - loss: 0.11 - ETA: 2:12:12 - loss: 0.11 - ETA: 2:12:03 - loss: 0.11 - ETA: 2:11:54 - loss: 0.10 - ETA: 2:11:45 - loss: 0.10 - ETA: 2:11:36 - loss: 0.10 - ETA: 2:11:27 - loss: 0.10 - ETA: 2:11:18 - loss: 0.10 - ETA: 2:11:09 - loss: 0.10 - ETA: 2:11:00 - loss: 0.10 - ETA: 2:10:51 - loss: 0.10 - ETA: 2:10:42 - loss: 0.10 - ETA: 2:10:33 - loss: 0.10 - ETA: 2:10:24 - loss: 0.10 - ETA: 2:10:16 - loss: 0.10 - ETA: 2:10:07 - loss: 0.10 - ETA: 2:09:58 - loss: 0.10 - ETA: 2:09:50 - loss: 0.10 - ETA: 2:09:43 - loss: 0.10 - ETA: 2:09:35 - loss: 0.10 - ETA: 2:09:26 - loss: 0.10 - ETA: 2:09:17 - loss: 0.10 - ETA: 2:09:08 - loss: 0.10 - ETA: 2:09:00 - loss: 0.10 - ETA: 2:08:51 - loss: 0.10 - ETA: 2:08:43 - loss: 0.10 - ETA: 2:08:35 - loss: 0.10 - ETA: 2:08:27 - loss: 0.10 - ETA: 2:08:18 - loss: 0.10 - ETA: 2:08:09 - loss: 0.10 - ETA: 2:08:01 - loss: 0.10 - ETA: 2:07:52 - loss: 0.10 - ETA: 2:07:44 - loss: 0.10 - ETA: 2:07:36 - loss: 0.10 - ETA: 2:07:27 - loss: 0.10 - ETA: 2:07:19 - loss: 0.10 - ETA: 2:07:11 - loss: 0.10 - ETA: 2:07:02 - loss: 0.10 - ETA: 2:06:55 - loss: 0.10 - ETA: 2:06:47 - loss: 0.10 - ETA: 2:06:38 - loss: 0.10 - ETA: 2:06:30 - loss: 0.10 - ETA: 2:06:22 - loss: 0.10 - ETA: 2:06:13 - loss: 0.10 - ETA: 2:06:05 - loss: 0.10 - ETA: 2:05:56 - loss: 0.10 - ETA: 2:05:48 - loss: 0.10 - ETA: 2:05:39 - loss: 0.10 - ETA: 2:05:30 - loss: 0.10 - ETA: 2:05:21 - loss: 0.10 - ETA: 2:05:13 - loss: 0.10 - ETA: 2:05:04 - loss: 0.10 - ETA: 2:04:55 - loss: 0.10 - ETA: 2:04:47 - loss: 0.10 - ETA: 2:04:39 - loss: 0.10 - ETA: 2:04:30 - loss: 0.10 - ETA: 2:04:22 - loss: 0.10 - ETA: 2:04:13 - loss: 0.10 - ETA: 2:04:04 - loss: 0.10 - ETA: 2:03:56 - loss: 0.10 - ETA: 2:03:47 - loss: 0.10 - ETA: 2:03:38 - loss: 0.10 - ETA: 2:03:30 - loss: 0.10 - ETA: 2:03:21 - loss: 0.10 - ETA: 2:03:12 - loss: 0.10 - ETA: 2:03:03 - loss: 0.10 - ETA: 2:02:55 - loss: 0.10 - ETA: 2:02:47 - loss: 0.10 - ETA: 2:02:38 - loss: 0.10 - ETA: 2:02:30 - loss: 0.10 - ETA: 2:02:22 - loss: 0.10 - ETA: 2:02:13 - loss: 0.10 - ETA: 2:02:04 - loss: 0.10 - ETA: 2:01:55 - loss: 0.10 - ETA: 2:01:48 - loss: 0.10 - ETA: 2:01:40 - loss: 0.10 - ETA: 2:01:32 - loss: 0.10 - ETA: 2:01:24 - loss: 0.10 - ETA: 2:01:16 - loss: 0.10 - ETA: 2:01:08 - loss: 0.10 - ETA: 2:01:00 - loss: 0.10 - ETA: 2:00:52 - loss: 0.10 - ETA: 2:00:43 - loss: 0.10 - ETA: 2:00:35 - loss: 0.10 - ETA: 2:00:27 - loss: 0.10 - ETA: 2:00:20 - loss: 0.10 - ETA: 2:00:12 - loss: 0.10 - ETA: 2:00:03 - loss: 0.10 - ETA: 1:59:55 - loss: 0.10 - ETA: 1:59:47 - loss: 0.10 - ETA: 1:59:39 - loss: 0.10 - ETA: 1:59:31 - loss: 0.10 - ETA: 1:59:23 - loss: 0.10 - ETA: 1:59:15 - loss: 0.10 - ETA: 1:59:06 - loss: 0.10 - ETA: 1:58:58 - loss: 0.10 - ETA: 1:58:50 - loss: 0.10 - ETA: 1:58:42 - loss: 0.10 - ETA: 1:58:33 - loss: 0.10 - ETA: 1:58:25 - loss: 0.10 - ETA: 1:58:17 - loss: 0.10 - ETA: 1:58:09 - loss: 0.10 - ETA: 1:58:00 - loss: 0.10 - ETA: 1:57:52 - loss: 0.10 - ETA: 1:57:45 - loss: 0.10 - ETA: 1:57:37 - loss: 0.10 - ETA: 1:57:29 - loss: 0.10 - ETA: 1:57:21 - loss: 0.10 - ETA: 1:57:13 - loss: 0.10 - ETA: 1:57:04 - loss: 0.10 - ETA: 1:56:56 - loss: 0.10 - ETA: 1:56:48 - loss: 0.10 - ETA: 1:56:39 - loss: 0.10 - ETA: 1:56:31 - loss: 0.10 - ETA: 1:56:23 - loss: 0.10 - ETA: 1:56:15 - loss: 0.10 - ETA: 1:56:07 - loss: 0.10 - ETA: 1:55:59 - loss: 0.10 - ETA: 1:55:51 - loss: 0.10 - ETA: 1:55:43 - loss: 0.10 - ETA: 1:55:35 - loss: 0.10 - ETA: 1:55:26 - loss: 0.10 - ETA: 1:55:18 - loss: 0.10 - ETA: 1:55:10 - loss: 0.10 - ETA: 1:55:02 - loss: 0.10 - ETA: 1:54:54 - loss: 0.10 - ETA: 1:54:46 - loss: 0.10 - ETA: 1:54:38 - loss: 0.10 - ETA: 1:54:30 - loss: 0.10 - ETA: 1:54:22 - loss: 0.10 - ETA: 1:54:16 - loss: 0.10 - ETA: 1:54:10 - loss: 0.10 - ETA: 1:54:02 - loss: 0.10 - ETA: 1:53:55 - loss: 0.10 - ETA: 1:53:47 - loss: 0.10 - ETA: 1:53:39 - loss: 0.10 - ETA: 1:53:32 - loss: 0.10 - ETA: 1:53:24 - loss: 0.10 - ETA: 1:53:17 - loss: 0.10 - ETA: 1:53:09 - loss: 0.10 - ETA: 1:53:01 - loss: 0.10 - ETA: 1:52:53 - loss: 0.10 - ETA: 1:52:45 - loss: 0.10 - ETA: 1:52:38 - loss: 0.10 - ETA: 1:52:30 - loss: 0.10 - ETA: 1:52:22 - loss: 0.10 - ETA: 1:52:14 - loss: 0.10 - ETA: 1:52:06 - loss: 0.10 - ETA: 1:51:58 - loss: 0.10 - ETA: 1:51:50 - loss: 0.10 - ETA: 1:51:42 - loss: 0.10 - ETA: 1:51:34 - loss: 0.10 - ETA: 1:51:26 - loss: 0.10 - ETA: 1:51:18 - loss: 0.10 - ETA: 1:51:09 - loss: 0.10 - ETA: 1:51:02 - loss: 0.10 - ETA: 1:50:54 - loss: 0.10 - ETA: 1:50:46 - loss: 0.10 - ETA: 1:50:38 - loss: 0.10 - ETA: 1:50:31 - loss: 0.10 - ETA: 1:50:24 - loss: 0.10 - ETA: 1:50:16 - loss: 0.10 - ETA: 1:50:08 - loss: 0.10 - ETA: 1:50:00 - loss: 0.10 - ETA: 1:49:53 - loss: 0.10 - ETA: 1:49:46 - loss: 0.10 - ETA: 1:49:38 - loss: 0.10 - ETA: 1:49:30 - loss: 0.10 - ETA: 1:49:22 - loss: 0.10 - ETA: 1:49:15 - loss: 0.10 - ETA: 1:49:08 - loss: 0.10 - ETA: 1:49:00 - loss: 0.10 - ETA: 1:48:53 - loss: 0.10 - ETA: 1:48:45 - loss: 0.10 - ETA: 1:48:38 - loss: 0.10 - ETA: 1:48:30 - loss: 0.10 - ETA: 1:48:22 - loss: 0.10 - ETA: 1:48:14 - loss: 0.10 - ETA: 1:48:06 - loss: 0.10 - ETA: 1:47:58 - loss: 0.10 - ETA: 1:47:50 - loss: 0.10 - ETA: 1:47:42 - loss: 0.10 - ETA: 1:47:35 - loss: 0.10 - ETA: 1:47:27 - loss: 0.10 - ETA: 1:47:20 - loss: 0.10 - ETA: 1:47:12 - loss: 0.10 - ETA: 1:47:04 - loss: 0.10 - ETA: 1:46:56 - loss: 0.10 - ETA: 1:46:48 - loss: 0.10 - ETA: 1:46:40 - loss: 0.10 - ETA: 1:46:32 - loss: 0.10 - ETA: 1:46:24 - loss: 0.10 - ETA: 1:46:16 - loss: 0.10 - ETA: 1:46:09 - loss: 0.10 - ETA: 1:46:01 - loss: 0.10 - ETA: 1:45:53 - loss: 0.10 - ETA: 1:45:45 - loss: 0.10 - ETA: 1:45:37 - loss: 0.10 - ETA: 1:45:29 - loss: 0.10 - ETA: 1:45:22 - loss: 0.10 - ETA: 1:45:14 - loss: 0.10 - ETA: 1:45:07 - loss: 0.10 - ETA: 1:44:59 - loss: 0.10 - ETA: 1:44:52 - loss: 0.10 - ETA: 1:44:45 - loss: 0.10 - ETA: 1:44:37 - loss: 0.10 - ETA: 1:44:31 - loss: 0.10 - ETA: 1:44:23 - loss: 0.10 - ETA: 1:44:15 - loss: 0.10 - ETA: 1:44:08 - loss: 0.10 - ETA: 1:44:00 - loss: 0.10 - ETA: 1:43:52 - loss: 0.10 - ETA: 1:43:45 - loss: 0.10 - ETA: 1:43:37 - loss: 0.10 - ETA: 1:43:30 - loss: 0.10 - ETA: 1:43:23 - loss: 0.10 - ETA: 1:43:16 - loss: 0.10 - ETA: 1:43:08 - loss: 0.10 - ETA: 1:43:00 - loss: 0.10 - ETA: 1:42:53 - loss: 0.10 - ETA: 1:42:45 - loss: 0.10 - ETA: 1:42:37 - loss: 0.10 - ETA: 1:42:30 - loss: 0.10 - ETA: 1:42:22 - loss: 0.10 - ETA: 1:42:14 - loss: 0.10 - ETA: 1:42:06 - loss: 0.10 - ETA: 1:41:59 - loss: 0.10 - ETA: 1:41:51 - loss: 0.10 - ETA: 1:41:43 - loss: 0.10 - ETA: 1:41:36 - loss: 0.10 - ETA: 1:41:28 - loss: 0.10 - ETA: 1:41:20 - loss: 0.10 - ETA: 1:41:13 - loss: 0.10 - ETA: 1:41:05 - loss: 0.10 - ETA: 1:40:57 - loss: 0.10 - ETA: 1:40:50 - loss: 0.10 - ETA: 1:40:42 - loss: 0.10 - ETA: 1:40:35 - loss: 0.10 - ETA: 1:40:27 - loss: 0.10 - ETA: 1:40:19 - loss: 0.10 - ETA: 1:40:12 - loss: 0.10 - ETA: 1:40:04 - loss: 0.10 - ETA: 1:39:56 - loss: 0.10 - ETA: 1:39:49 - loss: 0.10 - ETA: 1:39:41 - loss: 0.10 - ETA: 1:39:34 - loss: 0.10 - ETA: 1:39:26 - loss: 0.10 - ETA: 1:39:19 - loss: 0.10 - ETA: 1:39:11 - loss: 0.10 - ETA: 1:39:03 - loss: 0.10 - ETA: 1:38:56 - loss: 0.10 - ETA: 1:38:48 - loss: 0.10 - ETA: 1:38:41 - loss: 0.10 - ETA: 1:38:33 - loss: 0.10 - ETA: 1:38:26 - loss: 0.10 - ETA: 1:38:18 - loss: 0.10 - ETA: 1:38:11 - loss: 0.10 - ETA: 1:38:03 - loss: 0.10 - ETA: 1:37:56 - loss: 0.10 - ETA: 1:37:49 - loss: 0.10 - ETA: 1:37:42 - loss: 0.10 - ETA: 1:37:34 - loss: 0.10 - ETA: 1:37:27 - loss: 0.10 - ETA: 1:37:19 - loss: 0.10 - ETA: 1:37:12 - loss: 0.10 - ETA: 1:37:05 - loss: 0.10 - ETA: 1:36:58 - loss: 0.10 - ETA: 1:36:50 - loss: 0.10 - ETA: 1:36:43 - loss: 0.10 - ETA: 1:36:35 - loss: 0.1025111616/154783 [====================>.........] - ETA: 1:36:28 - loss: 0.10 - ETA: 1:36:20 - loss: 0.10 - ETA: 1:36:13 - loss: 0.10 - ETA: 1:36:06 - loss: 0.10 - ETA: 1:35:58 - loss: 0.10 - ETA: 1:35:51 - loss: 0.10 - ETA: 1:35:44 - loss: 0.10 - ETA: 1:35:37 - loss: 0.10 - ETA: 1:35:30 - loss: 0.10 - ETA: 1:35:22 - loss: 0.10 - ETA: 1:35:15 - loss: 0.10 - ETA: 1:35:08 - loss: 0.10 - ETA: 1:35:01 - loss: 0.10 - ETA: 1:34:53 - loss: 0.10 - ETA: 1:34:46 - loss: 0.10 - ETA: 1:34:39 - loss: 0.10 - ETA: 1:34:31 - loss: 0.10 - ETA: 1:34:24 - loss: 0.10 - ETA: 1:34:17 - loss: 0.10 - ETA: 1:34:09 - loss: 0.10 - ETA: 1:34:02 - loss: 0.10 - ETA: 1:33:55 - loss: 0.10 - ETA: 1:33:47 - loss: 0.10 - ETA: 1:33:40 - loss: 0.10 - ETA: 1:33:33 - loss: 0.10 - ETA: 1:33:26 - loss: 0.10 - ETA: 1:33:18 - loss: 0.10 - ETA: 1:33:11 - loss: 0.10 - ETA: 1:33:04 - loss: 0.10 - ETA: 1:32:57 - loss: 0.10 - ETA: 1:32:50 - loss: 0.10 - ETA: 1:32:43 - loss: 0.10 - ETA: 1:32:35 - loss: 0.10 - ETA: 1:32:28 - loss: 0.10 - ETA: 1:32:21 - loss: 0.10 - ETA: 1:32:14 - loss: 0.10 - ETA: 1:32:06 - loss: 0.10 - ETA: 1:31:59 - loss: 0.10 - ETA: 1:31:52 - loss: 0.10 - ETA: 1:31:45 - loss: 0.10 - ETA: 1:31:38 - loss: 0.10 - ETA: 1:31:30 - loss: 0.10 - ETA: 1:31:23 - loss: 0.10 - ETA: 1:31:16 - loss: 0.10 - ETA: 1:31:09 - loss: 0.10 - ETA: 1:31:02 - loss: 0.10 - ETA: 1:30:55 - loss: 0.10 - ETA: 1:30:47 - loss: 0.10 - ETA: 1:30:40 - loss: 0.10 - ETA: 1:30:33 - loss: 0.10 - ETA: 1:30:26 - loss: 0.10 - ETA: 1:30:19 - loss: 0.10 - ETA: 1:30:11 - loss: 0.10 - ETA: 1:30:04 - loss: 0.10 - ETA: 1:29:57 - loss: 0.10 - ETA: 1:29:50 - loss: 0.10 - ETA: 1:29:43 - loss: 0.10 - ETA: 1:29:36 - loss: 0.10 - ETA: 1:29:29 - loss: 0.10 - ETA: 1:29:22 - loss: 0.10 - ETA: 1:29:15 - loss: 0.10 - ETA: 1:29:08 - loss: 0.10 - ETA: 1:29:01 - loss: 0.10 - ETA: 1:28:54 - loss: 0.10 - ETA: 1:28:46 - loss: 0.10 - ETA: 1:28:39 - loss: 0.10 - ETA: 1:28:32 - loss: 0.10 - ETA: 1:28:25 - loss: 0.10 - ETA: 1:28:18 - loss: 0.10 - ETA: 1:28:11 - loss: 0.10 - ETA: 1:28:04 - loss: 0.10 - ETA: 1:27:57 - loss: 0.10 - ETA: 1:27:50 - loss: 0.10 - ETA: 1:27:43 - loss: 0.10 - ETA: 1:27:36 - loss: 0.10 - ETA: 1:27:29 - loss: 0.10 - ETA: 1:27:22 - loss: 0.10 - ETA: 1:27:15 - loss: 0.10 - ETA: 1:27:08 - loss: 0.10 - ETA: 1:27:01 - loss: 0.10 - ETA: 1:26:54 - loss: 0.10 - ETA: 1:26:47 - loss: 0.10 - ETA: 1:26:40 - loss: 0.10 - ETA: 1:26:33 - loss: 0.10 - ETA: 1:26:26 - loss: 0.10 - ETA: 1:26:19 - loss: 0.10 - ETA: 1:26:12 - loss: 0.10 - ETA: 1:26:05 - loss: 0.10 - ETA: 1:25:58 - loss: 0.10 - ETA: 1:25:51 - loss: 0.10 - ETA: 1:25:44 - loss: 0.10 - ETA: 1:25:37 - loss: 0.10 - ETA: 1:25:31 - loss: 0.10 - ETA: 1:25:24 - loss: 0.10 - ETA: 1:25:17 - loss: 0.10 - ETA: 1:25:11 - loss: 0.10 - ETA: 1:25:04 - loss: 0.10 - ETA: 1:24:58 - loss: 0.10 - ETA: 1:24:51 - loss: 0.10 - ETA: 1:24:44 - loss: 0.10 - ETA: 1:24:38 - loss: 0.10 - ETA: 1:24:31 - loss: 0.10 - ETA: 1:24:24 - loss: 0.10 - ETA: 1:24:17 - loss: 0.10 - ETA: 1:24:11 - loss: 0.10 - ETA: 1:24:04 - loss: 0.10 - ETA: 1:23:57 - loss: 0.10 - ETA: 1:23:50 - loss: 0.10 - ETA: 1:23:43 - loss: 0.10 - ETA: 1:23:36 - loss: 0.10 - ETA: 1:23:29 - loss: 0.10 - ETA: 1:23:22 - loss: 0.10 - ETA: 1:23:16 - loss: 0.10 - ETA: 1:23:09 - loss: 0.10 - ETA: 1:23:02 - loss: 0.10 - ETA: 1:22:55 - loss: 0.09 - ETA: 1:22:48 - loss: 0.09 - ETA: 1:22:41 - loss: 0.09 - ETA: 1:22:34 - loss: 0.09 - ETA: 1:22:27 - loss: 0.09 - ETA: 1:22:20 - loss: 0.09 - ETA: 1:22:13 - loss: 0.09 - ETA: 1:22:07 - loss: 0.09 - ETA: 1:22:00 - loss: 0.09 - ETA: 1:21:53 - loss: 0.09 - ETA: 1:21:46 - loss: 0.09 - ETA: 1:21:39 - loss: 0.09 - ETA: 1:21:32 - loss: 0.09 - ETA: 1:21:25 - loss: 0.09 - ETA: 1:21:18 - loss: 0.09 - ETA: 1:21:11 - loss: 0.09 - ETA: 1:21:05 - loss: 0.09 - ETA: 1:20:58 - loss: 0.09 - ETA: 1:20:51 - loss: 0.09 - ETA: 1:20:44 - loss: 0.09 - ETA: 1:20:37 - loss: 0.09 - ETA: 1:20:31 - loss: 0.09 - ETA: 1:20:24 - loss: 0.09 - ETA: 1:20:17 - loss: 0.09 - ETA: 1:20:10 - loss: 0.09 - ETA: 1:20:03 - loss: 0.09 - ETA: 1:19:56 - loss: 0.09 - ETA: 1:19:49 - loss: 0.09 - ETA: 1:19:43 - loss: 0.09 - ETA: 1:19:36 - loss: 0.09 - ETA: 1:19:29 - loss: 0.09 - ETA: 1:19:22 - loss: 0.09 - ETA: 1:19:15 - loss: 0.09 - ETA: 1:19:09 - loss: 0.09 - ETA: 1:19:02 - loss: 0.09 - ETA: 1:18:55 - loss: 0.09 - ETA: 1:18:48 - loss: 0.09 - ETA: 1:18:41 - loss: 0.09 - ETA: 1:18:35 - loss: 0.09 - ETA: 1:18:28 - loss: 0.09 - ETA: 1:18:21 - loss: 0.09 - ETA: 1:18:14 - loss: 0.09 - ETA: 1:18:08 - loss: 0.09 - ETA: 1:18:01 - loss: 0.09 - ETA: 1:17:54 - loss: 0.09 - ETA: 1:17:48 - loss: 0.09 - ETA: 1:17:41 - loss: 0.09 - ETA: 1:17:34 - loss: 0.09 - ETA: 1:17:27 - loss: 0.09 - ETA: 1:17:21 - loss: 0.09 - ETA: 1:17:14 - loss: 0.09 - ETA: 1:17:07 - loss: 0.09 - ETA: 1:17:01 - loss: 0.09 - ETA: 1:16:54 - loss: 0.09 - ETA: 1:16:48 - loss: 0.09 - ETA: 1:16:42 - loss: 0.09 - ETA: 1:16:35 - loss: 0.09 - ETA: 1:16:29 - loss: 0.09 - ETA: 1:16:22 - loss: 0.09 - ETA: 1:16:15 - loss: 0.09 - ETA: 1:16:09 - loss: 0.09 - ETA: 1:16:02 - loss: 0.09 - ETA: 1:15:55 - loss: 0.09 - ETA: 1:15:49 - loss: 0.09 - ETA: 1:15:42 - loss: 0.09 - ETA: 1:15:36 - loss: 0.09 - ETA: 1:15:29 - loss: 0.09 - ETA: 1:15:23 - loss: 0.09 - ETA: 1:15:16 - loss: 0.09 - ETA: 1:15:10 - loss: 0.09 - ETA: 1:15:03 - loss: 0.09 - ETA: 1:14:57 - loss: 0.09 - ETA: 1:14:50 - loss: 0.09 - ETA: 1:14:43 - loss: 0.09 - ETA: 1:14:37 - loss: 0.09 - ETA: 1:14:31 - loss: 0.09 - ETA: 1:14:24 - loss: 0.09 - ETA: 1:14:18 - loss: 0.09 - ETA: 1:14:11 - loss: 0.09 - ETA: 1:14:05 - loss: 0.09 - ETA: 1:13:58 - loss: 0.09 - ETA: 1:13:51 - loss: 0.09 - ETA: 1:13:45 - loss: 0.09 - ETA: 1:13:39 - loss: 0.09 - ETA: 1:13:32 - loss: 0.09 - ETA: 1:13:26 - loss: 0.09 - ETA: 1:13:19 - loss: 0.09 - ETA: 1:13:13 - loss: 0.09 - ETA: 1:13:07 - loss: 0.09 - ETA: 1:13:00 - loss: 0.09 - ETA: 1:12:54 - loss: 0.09 - ETA: 1:12:48 - loss: 0.09 - ETA: 1:12:41 - loss: 0.09 - ETA: 1:12:34 - loss: 0.09 - ETA: 1:12:28 - loss: 0.09 - ETA: 1:12:21 - loss: 0.09 - ETA: 1:12:15 - loss: 0.09 - ETA: 1:12:08 - loss: 0.09 - ETA: 1:12:02 - loss: 0.09 - ETA: 1:11:56 - loss: 0.09 - ETA: 1:11:49 - loss: 0.09 - ETA: 1:11:43 - loss: 0.09 - ETA: 1:11:36 - loss: 0.09 - ETA: 1:11:30 - loss: 0.09 - ETA: 1:11:24 - loss: 0.09 - ETA: 1:11:17 - loss: 0.09 - ETA: 1:11:11 - loss: 0.09 - ETA: 1:11:04 - loss: 0.09 - ETA: 1:10:58 - loss: 0.09 - ETA: 1:10:51 - loss: 0.09 - ETA: 1:10:45 - loss: 0.09 - ETA: 1:10:39 - loss: 0.09 - ETA: 1:10:32 - loss: 0.09 - ETA: 1:10:26 - loss: 0.09 - ETA: 1:10:19 - loss: 0.09 - ETA: 1:10:12 - loss: 0.09 - ETA: 1:10:06 - loss: 0.09 - ETA: 1:09:59 - loss: 0.09 - ETA: 1:09:53 - loss: 0.09 - ETA: 1:09:46 - loss: 0.09 - ETA: 1:09:40 - loss: 0.09 - ETA: 1:09:33 - loss: 0.09 - ETA: 1:09:27 - loss: 0.09 - ETA: 1:09:21 - loss: 0.09 - ETA: 1:09:15 - loss: 0.09 - ETA: 1:09:09 - loss: 0.09 - ETA: 1:09:03 - loss: 0.09 - ETA: 1:08:57 - loss: 0.09 - ETA: 1:08:51 - loss: 0.09 - ETA: 1:08:46 - loss: 0.09 - ETA: 1:08:40 - loss: 0.09 - ETA: 1:08:35 - loss: 0.09 - ETA: 1:08:29 - loss: 0.09 - ETA: 1:08:23 - loss: 0.09 - ETA: 1:08:17 - loss: 0.09 - ETA: 1:08:10 - loss: 0.09 - ETA: 1:08:04 - loss: 0.09 - ETA: 1:07:58 - loss: 0.09 - ETA: 1:07:51 - loss: 0.09 - ETA: 1:07:45 - loss: 0.09 - ETA: 1:07:39 - loss: 0.09 - ETA: 1:07:33 - loss: 0.09 - ETA: 1:07:27 - loss: 0.09 - ETA: 1:07:21 - loss: 0.09 - ETA: 1:07:14 - loss: 0.09 - ETA: 1:07:08 - loss: 0.09 - ETA: 1:07:02 - loss: 0.09 - ETA: 1:06:56 - loss: 0.09 - ETA: 1:06:50 - loss: 0.09 - ETA: 1:06:44 - loss: 0.09 - ETA: 1:06:38 - loss: 0.09 - ETA: 1:06:31 - loss: 0.09 - ETA: 1:06:25 - loss: 0.09 - ETA: 1:06:19 - loss: 0.09 - ETA: 1:06:12 - loss: 0.09 - ETA: 1:06:06 - loss: 0.09 - ETA: 1:06:00 - loss: 0.09 - ETA: 1:05:54 - loss: 0.09 - ETA: 1:05:48 - loss: 0.09 - ETA: 1:05:42 - loss: 0.09 - ETA: 1:05:36 - loss: 0.09 - ETA: 1:05:30 - loss: 0.09 - ETA: 1:05:23 - loss: 0.09 - ETA: 1:05:17 - loss: 0.09 - ETA: 1:05:11 - loss: 0.09 - ETA: 1:05:05 - loss: 0.09 - ETA: 1:04:59 - loss: 0.09 - ETA: 1:04:53 - loss: 0.09 - ETA: 1:04:46 - loss: 0.09 - ETA: 1:04:40 - loss: 0.09 - ETA: 1:04:33 - loss: 0.09 - ETA: 1:04:27 - loss: 0.09 - ETA: 1:04:21 - loss: 0.09 - ETA: 1:04:15 - loss: 0.09 - ETA: 1:04:09 - loss: 0.09 - ETA: 1:04:02 - loss: 0.0965131520/154783 [========================>.....] - ETA: 1:03:56 - loss: 0.09 - ETA: 1:03:50 - loss: 0.09 - ETA: 1:03:44 - loss: 0.09 - ETA: 1:03:37 - loss: 0.09 - ETA: 1:03:31 - loss: 0.09 - ETA: 1:03:25 - loss: 0.09 - ETA: 1:03:18 - loss: 0.09 - ETA: 1:03:12 - loss: 0.09 - ETA: 1:03:07 - loss: 0.09 - ETA: 1:03:01 - loss: 0.09 - ETA: 1:02:55 - loss: 0.09 - ETA: 1:02:48 - loss: 0.09 - ETA: 1:02:42 - loss: 0.09 - ETA: 1:02:36 - loss: 0.09 - ETA: 1:02:30 - loss: 0.09 - ETA: 1:02:24 - loss: 0.09 - ETA: 1:02:18 - loss: 0.09 - ETA: 1:02:12 - loss: 0.09 - ETA: 1:02:06 - loss: 0.09 - ETA: 1:01:59 - loss: 0.09 - ETA: 1:01:53 - loss: 0.09 - ETA: 1:01:47 - loss: 0.09 - ETA: 1:01:41 - loss: 0.09 - ETA: 1:01:35 - loss: 0.09 - ETA: 1:01:28 - loss: 0.09 - ETA: 1:01:22 - loss: 0.09 - ETA: 1:01:16 - loss: 0.09 - ETA: 1:01:10 - loss: 0.09 - ETA: 1:01:04 - loss: 0.09 - ETA: 1:00:57 - loss: 0.09 - ETA: 1:00:51 - loss: 0.09 - ETA: 1:00:45 - loss: 0.09 - ETA: 1:00:39 - loss: 0.09 - ETA: 1:00:32 - loss: 0.09 - ETA: 1:00:26 - loss: 0.09 - ETA: 1:00:20 - loss: 0.09 - ETA: 1:00:14 - loss: 0.09 - ETA: 1:00:07 - loss: 0.09 - ETA: 1:00:01 - loss: 0.09 - ETA: 59:55 - loss: 0.0958 - ETA: 59:49 - loss: 0.09 - ETA: 59:43 - loss: 0.09 - ETA: 59:37 - loss: 0.09 - ETA: 59:30 - loss: 0.09 - ETA: 59:24 - loss: 0.09 - ETA: 59:18 - loss: 0.09 - ETA: 59:12 - loss: 0.09 - ETA: 59:05 - loss: 0.09 - ETA: 59:00 - loss: 0.09 - ETA: 58:54 - loss: 0.09 - ETA: 58:49 - loss: 0.09 - ETA: 58:43 - loss: 0.09 - ETA: 58:37 - loss: 0.09 - ETA: 58:31 - loss: 0.09 - ETA: 58:25 - loss: 0.09 - ETA: 58:19 - loss: 0.09 - ETA: 58:13 - loss: 0.09 - ETA: 58:07 - loss: 0.09 - ETA: 58:01 - loss: 0.09 - ETA: 57:55 - loss: 0.09 - ETA: 57:49 - loss: 0.09 - ETA: 57:43 - loss: 0.09 - ETA: 57:37 - loss: 0.09 - ETA: 57:31 - loss: 0.09 - ETA: 57:25 - loss: 0.09 - ETA: 57:19 - loss: 0.09 - ETA: 57:13 - loss: 0.09 - ETA: 57:06 - loss: 0.09 - ETA: 57:01 - loss: 0.09 - ETA: 56:55 - loss: 0.09 - ETA: 56:49 - loss: 0.09 - ETA: 56:43 - loss: 0.09 - ETA: 56:37 - loss: 0.09 - ETA: 56:31 - loss: 0.09 - ETA: 56:24 - loss: 0.09 - ETA: 56:18 - loss: 0.09 - ETA: 56:12 - loss: 0.09 - ETA: 56:06 - loss: 0.09 - ETA: 56:00 - loss: 0.09 - ETA: 55:54 - loss: 0.09 - ETA: 55:48 - loss: 0.09 - ETA: 55:42 - loss: 0.09 - ETA: 55:36 - loss: 0.09 - ETA: 55:31 - loss: 0.09 - ETA: 55:25 - loss: 0.09 - ETA: 55:19 - loss: 0.09 - ETA: 55:13 - loss: 0.09 - ETA: 55:06 - loss: 0.09 - ETA: 55:00 - loss: 0.09 - ETA: 54:54 - loss: 0.09 - ETA: 54:48 - loss: 0.09 - ETA: 54:42 - loss: 0.09 - ETA: 54:36 - loss: 0.09 - ETA: 54:30 - loss: 0.09 - ETA: 54:24 - loss: 0.09 - ETA: 54:18 - loss: 0.09 - ETA: 54:12 - loss: 0.09 - ETA: 54:07 - loss: 0.09 - ETA: 54:01 - loss: 0.09 - ETA: 53:55 - loss: 0.09 - ETA: 53:50 - loss: 0.09 - ETA: 53:44 - loss: 0.09 - ETA: 53:38 - loss: 0.09 - ETA: 53:32 - loss: 0.09 - ETA: 53:27 - loss: 0.09 - ETA: 53:21 - loss: 0.09 - ETA: 53:15 - loss: 0.09 - ETA: 53:10 - loss: 0.09 - ETA: 53:04 - loss: 0.09 - ETA: 52:59 - loss: 0.09 - ETA: 52:53 - loss: 0.09 - ETA: 52:48 - loss: 0.09 - ETA: 52:42 - loss: 0.09 - ETA: 52:36 - loss: 0.09 - ETA: 52:30 - loss: 0.09 - ETA: 52:24 - loss: 0.09 - ETA: 52:18 - loss: 0.09 - ETA: 52:13 - loss: 0.09 - ETA: 52:08 - loss: 0.09 - ETA: 52:02 - loss: 0.09 - ETA: 51:56 - loss: 0.09 - ETA: 51:51 - loss: 0.09 - ETA: 51:45 - loss: 0.09 - ETA: 51:39 - loss: 0.09 - ETA: 51:33 - loss: 0.09 - ETA: 51:27 - loss: 0.09 - ETA: 51:21 - loss: 0.09 - ETA: 51:15 - loss: 0.09 - ETA: 51:09 - loss: 0.09 - ETA: 51:03 - loss: 0.09 - ETA: 50:57 - loss: 0.09 - ETA: 50:51 - loss: 0.09 - ETA: 50:45 - loss: 0.09 - ETA: 50:39 - loss: 0.09 - ETA: 50:33 - loss: 0.09 - ETA: 50:28 - loss: 0.09 - ETA: 50:23 - loss: 0.09 - ETA: 50:17 - loss: 0.09 - ETA: 50:11 - loss: 0.09 - ETA: 50:06 - loss: 0.09 - ETA: 50:00 - loss: 0.09 - ETA: 49:55 - loss: 0.09 - ETA: 49:49 - loss: 0.09 - ETA: 49:44 - loss: 0.09 - ETA: 49:38 - loss: 0.09 - ETA: 49:33 - loss: 0.09 - ETA: 49:27 - loss: 0.09 - ETA: 49:21 - loss: 0.09 - ETA: 49:15 - loss: 0.09 - ETA: 49:09 - loss: 0.09 - ETA: 49:04 - loss: 0.09 - ETA: 48:57 - loss: 0.09 - ETA: 48:52 - loss: 0.09 - ETA: 48:46 - loss: 0.09 - ETA: 48:41 - loss: 0.09 - ETA: 48:35 - loss: 0.09 - ETA: 48:30 - loss: 0.09 - ETA: 48:24 - loss: 0.09 - ETA: 48:18 - loss: 0.09 - ETA: 48:13 - loss: 0.09 - ETA: 48:07 - loss: 0.09 - ETA: 48:02 - loss: 0.09 - ETA: 47:56 - loss: 0.09 - ETA: 47:50 - loss: 0.09 - ETA: 47:45 - loss: 0.09 - ETA: 47:39 - loss: 0.09 - ETA: 47:33 - loss: 0.09 - ETA: 47:27 - loss: 0.09 - ETA: 47:21 - loss: 0.09 - ETA: 47:16 - loss: 0.09 - ETA: 47:11 - loss: 0.09 - ETA: 47:06 - loss: 0.09 - ETA: 47:00 - loss: 0.09 - ETA: 46:55 - loss: 0.09 - ETA: 46:50 - loss: 0.09 - ETA: 46:44 - loss: 0.09 - ETA: 46:39 - loss: 0.09 - ETA: 46:33 - loss: 0.09 - ETA: 46:27 - loss: 0.09 - ETA: 46:22 - loss: 0.09 - ETA: 46:16 - loss: 0.09 - ETA: 46:11 - loss: 0.09 - ETA: 46:05 - loss: 0.09 - ETA: 45:59 - loss: 0.09 - ETA: 45:54 - loss: 0.09 - ETA: 45:48 - loss: 0.09 - ETA: 45:43 - loss: 0.09 - ETA: 45:37 - loss: 0.09 - ETA: 45:32 - loss: 0.09 - ETA: 45:26 - loss: 0.09 - ETA: 45:21 - loss: 0.09 - ETA: 45:15 - loss: 0.09 - ETA: 45:09 - loss: 0.09 - ETA: 45:03 - loss: 0.09 - ETA: 44:57 - loss: 0.09 - ETA: 44:52 - loss: 0.09 - ETA: 44:46 - loss: 0.09 - ETA: 44:41 - loss: 0.09 - ETA: 44:35 - loss: 0.09 - ETA: 44:29 - loss: 0.09 - ETA: 44:23 - loss: 0.09 - ETA: 44:17 - loss: 0.09 - ETA: 44:11 - loss: 0.09 - ETA: 44:05 - loss: 0.09 - ETA: 43:59 - loss: 0.09 - ETA: 43:54 - loss: 0.09 - ETA: 43:48 - loss: 0.09 - ETA: 43:43 - loss: 0.09 - ETA: 43:37 - loss: 0.09 - ETA: 43:31 - loss: 0.09 - ETA: 43:26 - loss: 0.09 - ETA: 43:20 - loss: 0.09 - ETA: 43:14 - loss: 0.09 - ETA: 43:09 - loss: 0.09 - ETA: 43:03 - loss: 0.09 - ETA: 42:57 - loss: 0.09 - ETA: 42:51 - loss: 0.09 - ETA: 42:46 - loss: 0.09 - ETA: 42:40 - loss: 0.09 - ETA: 42:35 - loss: 0.09 - ETA: 42:29 - loss: 0.09 - ETA: 42:23 - loss: 0.09 - ETA: 42:18 - loss: 0.09 - ETA: 42:12 - loss: 0.09 - ETA: 42:07 - loss: 0.09 - ETA: 42:01 - loss: 0.09 - ETA: 41:56 - loss: 0.09 - ETA: 41:50 - loss: 0.09 - ETA: 41:44 - loss: 0.09 - ETA: 41:38 - loss: 0.09 - ETA: 41:32 - loss: 0.09 - ETA: 41:27 - loss: 0.09 - ETA: 41:21 - loss: 0.09 - ETA: 41:15 - loss: 0.09 - ETA: 41:09 - loss: 0.09 - ETA: 41:04 - loss: 0.09 - ETA: 40:58 - loss: 0.09 - ETA: 40:52 - loss: 0.09 - ETA: 40:46 - loss: 0.09 - ETA: 40:41 - loss: 0.09 - ETA: 40:35 - loss: 0.09 - ETA: 40:29 - loss: 0.09 - ETA: 40:23 - loss: 0.09 - ETA: 40:18 - loss: 0.09 - ETA: 40:12 - loss: 0.09 - ETA: 40:06 - loss: 0.09 - ETA: 40:01 - loss: 0.09 - ETA: 39:55 - loss: 0.09 - ETA: 39:49 - loss: 0.09 - ETA: 39:44 - loss: 0.09 - ETA: 39:38 - loss: 0.09 - ETA: 39:33 - loss: 0.09 - ETA: 39:27 - loss: 0.09 - ETA: 39:21 - loss: 0.09 - ETA: 39:16 - loss: 0.09 - ETA: 39:10 - loss: 0.09 - ETA: 39:04 - loss: 0.09 - ETA: 38:58 - loss: 0.09 - ETA: 38:53 - loss: 0.09 - ETA: 38:47 - loss: 0.09 - ETA: 38:41 - loss: 0.09 - ETA: 38:35 - loss: 0.09 - ETA: 38:30 - loss: 0.09 - ETA: 38:24 - loss: 0.09 - ETA: 38:18 - loss: 0.09 - ETA: 38:13 - loss: 0.09 - ETA: 38:07 - loss: 0.09 - ETA: 38:01 - loss: 0.09 - ETA: 37:56 - loss: 0.09 - ETA: 37:50 - loss: 0.09 - ETA: 37:44 - loss: 0.09 - ETA: 37:38 - loss: 0.09 - ETA: 37:33 - loss: 0.09 - ETA: 37:27 - loss: 0.09 - ETA: 37:22 - loss: 0.09 - ETA: 37:16 - loss: 0.09 - ETA: 37:10 - loss: 0.09 - ETA: 37:05 - loss: 0.09 - ETA: 36:59 - loss: 0.09 - ETA: 36:53 - loss: 0.09 - ETA: 36:47 - loss: 0.09 - ETA: 36:42 - loss: 0.09 - ETA: 36:36 - loss: 0.09 - ETA: 36:30 - loss: 0.09 - ETA: 36:25 - loss: 0.09 - ETA: 36:19 - loss: 0.09 - ETA: 36:13 - loss: 0.09 - ETA: 36:07 - loss: 0.09 - ETA: 36:01 - loss: 0.09 - ETA: 35:56 - loss: 0.09 - ETA: 35:50 - loss: 0.09 - ETA: 35:45 - loss: 0.09 - ETA: 35:39 - loss: 0.09 - ETA: 35:33 - loss: 0.09 - ETA: 35:27 - loss: 0.09 - ETA: 35:22 - loss: 0.09 - ETA: 35:16 - loss: 0.09 - ETA: 35:11 - loss: 0.09 - ETA: 35:05 - loss: 0.09 - ETA: 34:59 - loss: 0.09 - ETA: 34:54 - loss: 0.09 - ETA: 34:48 - loss: 0.09 - ETA: 34:42 - loss: 0.09 - ETA: 34:37 - loss: 0.09 - ETA: 34:31 - loss: 0.09 - ETA: 34:25 - loss: 0.09 - ETA: 34:19 - loss: 0.09 - ETA: 34:14 - loss: 0.09 - ETA: 34:08 - loss: 0.09 - ETA: 34:03 - loss: 0.09 - ETA: 33:57 - loss: 0.0916151616/154783 [============================>.] - ETA: 33:52 - loss: 0.09 - ETA: 33:46 - loss: 0.09 - ETA: 33:41 - loss: 0.09 - ETA: 33:35 - loss: 0.09 - ETA: 33:30 - loss: 0.09 - ETA: 33:24 - loss: 0.09 - ETA: 33:18 - loss: 0.09 - ETA: 33:13 - loss: 0.09 - ETA: 33:07 - loss: 0.09 - ETA: 33:01 - loss: 0.09 - ETA: 32:56 - loss: 0.09 - ETA: 32:50 - loss: 0.09 - ETA: 32:44 - loss: 0.09 - ETA: 32:38 - loss: 0.09 - ETA: 32:33 - loss: 0.09 - ETA: 32:27 - loss: 0.09 - ETA: 32:21 - loss: 0.09 - ETA: 32:16 - loss: 0.09 - ETA: 32:10 - loss: 0.09 - ETA: 32:04 - loss: 0.09 - ETA: 31:59 - loss: 0.09 - ETA: 31:53 - loss: 0.09 - ETA: 31:47 - loss: 0.09 - ETA: 31:42 - loss: 0.09 - ETA: 31:36 - loss: 0.09 - ETA: 31:30 - loss: 0.09 - ETA: 31:25 - loss: 0.09 - ETA: 31:19 - loss: 0.09 - ETA: 31:13 - loss: 0.09 - ETA: 31:08 - loss: 0.09 - ETA: 31:02 - loss: 0.09 - ETA: 30:56 - loss: 0.09 - ETA: 30:51 - loss: 0.09 - ETA: 30:45 - loss: 0.09 - ETA: 30:39 - loss: 0.09 - ETA: 30:33 - loss: 0.09 - ETA: 30:28 - loss: 0.09 - ETA: 30:22 - loss: 0.09 - ETA: 30:16 - loss: 0.09 - ETA: 30:11 - loss: 0.09 - ETA: 30:05 - loss: 0.09 - ETA: 29:59 - loss: 0.09 - ETA: 29:54 - loss: 0.09 - ETA: 29:48 - loss: 0.09 - ETA: 29:43 - loss: 0.09 - ETA: 29:38 - loss: 0.09 - ETA: 29:32 - loss: 0.09 - ETA: 29:27 - loss: 0.09 - ETA: 29:21 - loss: 0.09 - ETA: 29:15 - loss: 0.09 - ETA: 29:10 - loss: 0.09 - ETA: 29:04 - loss: 0.09 - ETA: 28:59 - loss: 0.09 - ETA: 28:53 - loss: 0.09 - ETA: 28:48 - loss: 0.09 - ETA: 28:42 - loss: 0.09 - ETA: 28:36 - loss: 0.09 - ETA: 28:30 - loss: 0.09 - ETA: 28:25 - loss: 0.09 - ETA: 28:19 - loss: 0.09 - ETA: 28:14 - loss: 0.09 - ETA: 28:08 - loss: 0.09 - ETA: 28:02 - loss: 0.09 - ETA: 27:57 - loss: 0.09 - ETA: 27:51 - loss: 0.09 - ETA: 27:45 - loss: 0.09 - ETA: 27:39 - loss: 0.09 - ETA: 27:34 - loss: 0.09 - ETA: 27:28 - loss: 0.09 - ETA: 27:22 - loss: 0.09 - ETA: 27:16 - loss: 0.09 - ETA: 27:11 - loss: 0.09 - ETA: 27:05 - loss: 0.09 - ETA: 26:59 - loss: 0.09 - ETA: 26:53 - loss: 0.09 - ETA: 26:47 - loss: 0.09 - ETA: 26:42 - loss: 0.09 - ETA: 26:36 - loss: 0.09 - ETA: 26:30 - loss: 0.09 - ETA: 26:24 - loss: 0.09 - ETA: 26:18 - loss: 0.09 - ETA: 26:12 - loss: 0.09 - ETA: 26:07 - loss: 0.09 - ETA: 26:01 - loss: 0.09 - ETA: 25:55 - loss: 0.09 - ETA: 25:49 - loss: 0.09 - ETA: 25:43 - loss: 0.09 - ETA: 25:38 - loss: 0.09 - ETA: 25:32 - loss: 0.09 - ETA: 25:26 - loss: 0.09 - ETA: 25:20 - loss: 0.09 - ETA: 25:15 - loss: 0.09 - ETA: 25:09 - loss: 0.09 - ETA: 25:03 - loss: 0.09 - ETA: 24:58 - loss: 0.09 - ETA: 24:52 - loss: 0.09 - ETA: 24:46 - loss: 0.09 - ETA: 24:40 - loss: 0.09 - ETA: 24:35 - loss: 0.09 - ETA: 24:29 - loss: 0.09 - ETA: 24:23 - loss: 0.09 - ETA: 24:18 - loss: 0.09 - ETA: 24:12 - loss: 0.09 - ETA: 24:06 - loss: 0.09 - ETA: 24:01 - loss: 0.09 - ETA: 23:55 - loss: 0.09 - ETA: 23:49 - loss: 0.09 - ETA: 23:43 - loss: 0.09 - ETA: 23:38 - loss: 0.09 - ETA: 23:32 - loss: 0.09 - ETA: 23:26 - loss: 0.09 - ETA: 23:21 - loss: 0.09 - ETA: 23:15 - loss: 0.09 - ETA: 23:09 - loss: 0.09 - ETA: 23:03 - loss: 0.09 - ETA: 22:58 - loss: 0.09 - ETA: 22:52 - loss: 0.09 - ETA: 22:46 - loss: 0.09 - ETA: 22:40 - loss: 0.09 - ETA: 22:35 - loss: 0.09 - ETA: 22:29 - loss: 0.09 - ETA: 22:23 - loss: 0.08 - ETA: 22:18 - loss: 0.09 - ETA: 22:12 - loss: 0.08 - ETA: 22:06 - loss: 0.08 - ETA: 22:00 - loss: 0.08 - ETA: 21:55 - loss: 0.08 - ETA: 21:49 - loss: 0.08 - ETA: 21:43 - loss: 0.08 - ETA: 21:38 - loss: 0.08 - ETA: 21:32 - loss: 0.08 - ETA: 21:26 - loss: 0.08 - ETA: 21:21 - loss: 0.08 - ETA: 21:15 - loss: 0.08 - ETA: 21:09 - loss: 0.08 - ETA: 21:04 - loss: 0.08 - ETA: 20:58 - loss: 0.08 - ETA: 20:52 - loss: 0.08 - ETA: 20:46 - loss: 0.08 - ETA: 20:41 - loss: 0.08 - ETA: 20:35 - loss: 0.08 - ETA: 20:30 - loss: 0.08 - ETA: 20:25 - loss: 0.08 - ETA: 20:19 - loss: 0.08 - ETA: 20:13 - loss: 0.08 - ETA: 20:08 - loss: 0.08 - ETA: 20:02 - loss: 0.08 - ETA: 19:56 - loss: 0.08 - ETA: 19:51 - loss: 0.08 - ETA: 19:45 - loss: 0.08 - ETA: 19:40 - loss: 0.08 - ETA: 19:34 - loss: 0.08 - ETA: 19:28 - loss: 0.08 - ETA: 19:23 - loss: 0.08 - ETA: 19:17 - loss: 0.08 - ETA: 19:12 - loss: 0.08 - ETA: 19:06 - loss: 0.08 - ETA: 19:01 - loss: 0.08 - ETA: 18:55 - loss: 0.08 - ETA: 18:50 - loss: 0.08 - ETA: 18:44 - loss: 0.08 - ETA: 18:39 - loss: 0.08 - ETA: 18:33 - loss: 0.08 - ETA: 18:27 - loss: 0.08 - ETA: 18:22 - loss: 0.08 - ETA: 18:16 - loss: 0.08 - ETA: 18:11 - loss: 0.08 - ETA: 18:05 - loss: 0.08 - ETA: 17:59 - loss: 0.08 - ETA: 17:54 - loss: 0.08 - ETA: 17:48 - loss: 0.08 - ETA: 17:43 - loss: 0.08 - ETA: 17:37 - loss: 0.08 - ETA: 17:32 - loss: 0.08 - ETA: 17:26 - loss: 0.08 - ETA: 17:21 - loss: 0.08 - ETA: 17:15 - loss: 0.08 - ETA: 17:10 - loss: 0.08 - ETA: 17:04 - loss: 0.08 - ETA: 16:58 - loss: 0.08 - ETA: 16:53 - loss: 0.08 - ETA: 16:47 - loss: 0.08 - ETA: 16:41 - loss: 0.08 - ETA: 16:36 - loss: 0.08 - ETA: 16:30 - loss: 0.08 - ETA: 16:25 - loss: 0.08 - ETA: 16:19 - loss: 0.08 - ETA: 16:14 - loss: 0.08 - ETA: 16:08 - loss: 0.08 - ETA: 16:03 - loss: 0.08 - ETA: 15:57 - loss: 0.08 - ETA: 15:52 - loss: 0.08 - ETA: 15:46 - loss: 0.08 - ETA: 15:41 - loss: 0.08 - ETA: 15:35 - loss: 0.08 - ETA: 15:30 - loss: 0.08 - ETA: 15:24 - loss: 0.08 - ETA: 15:19 - loss: 0.08 - ETA: 15:13 - loss: 0.08 - ETA: 15:08 - loss: 0.08 - ETA: 15:02 - loss: 0.08 - ETA: 14:57 - loss: 0.08 - ETA: 14:51 - loss: 0.08 - ETA: 14:45 - loss: 0.08 - ETA: 14:40 - loss: 0.08 - ETA: 14:34 - loss: 0.08 - ETA: 14:29 - loss: 0.08 - ETA: 14:23 - loss: 0.08 - ETA: 14:18 - loss: 0.08 - ETA: 14:12 - loss: 0.08 - ETA: 14:06 - loss: 0.08 - ETA: 14:01 - loss: 0.08 - ETA: 13:55 - loss: 0.08 - ETA: 13:50 - loss: 0.08 - ETA: 13:44 - loss: 0.08 - ETA: 13:39 - loss: 0.08 - ETA: 13:33 - loss: 0.08 - ETA: 13:28 - loss: 0.08 - ETA: 13:22 - loss: 0.08 - ETA: 13:17 - loss: 0.08 - ETA: 13:11 - loss: 0.08 - ETA: 13:05 - loss: 0.08 - ETA: 13:00 - loss: 0.08 - ETA: 12:54 - loss: 0.08 - ETA: 12:49 - loss: 0.08 - ETA: 12:43 - loss: 0.08 - ETA: 12:38 - loss: 0.08 - ETA: 12:32 - loss: 0.08 - ETA: 12:27 - loss: 0.08 - ETA: 12:21 - loss: 0.08 - ETA: 12:16 - loss: 0.08 - ETA: 12:10 - loss: 0.08 - ETA: 12:04 - loss: 0.08 - ETA: 11:59 - loss: 0.08 - ETA: 11:53 - loss: 0.08 - ETA: 11:48 - loss: 0.08 - ETA: 11:42 - loss: 0.08 - ETA: 11:37 - loss: 0.08 - ETA: 11:31 - loss: 0.08 - ETA: 11:26 - loss: 0.08 - ETA: 11:20 - loss: 0.08 - ETA: 11:14 - loss: 0.08 - ETA: 11:09 - loss: 0.08 - ETA: 11:03 - loss: 0.08 - ETA: 10:58 - loss: 0.08 - ETA: 10:52 - loss: 0.08 - ETA: 10:46 - loss: 0.08 - ETA: 10:41 - loss: 0.08 - ETA: 10:35 - loss: 0.08 - ETA: 10:30 - loss: 0.08 - ETA: 10:24 - loss: 0.08 - ETA: 10:19 - loss: 0.08 - ETA: 10:13 - loss: 0.08 - ETA: 10:08 - loss: 0.08 - ETA: 10:02 - loss: 0.08 - ETA: 9:56 - loss: 0.0884 - ETA: 9:51 - loss: 0.088 - ETA: 9:45 - loss: 0.088 - ETA: 9:40 - loss: 0.088 - ETA: 9:34 - loss: 0.088 - ETA: 9:29 - loss: 0.088 - ETA: 9:23 - loss: 0.088 - ETA: 9:18 - loss: 0.088 - ETA: 9:12 - loss: 0.088 - ETA: 9:06 - loss: 0.088 - ETA: 9:01 - loss: 0.088 - ETA: 8:55 - loss: 0.088 - ETA: 8:50 - loss: 0.088 - ETA: 8:44 - loss: 0.088 - ETA: 8:39 - loss: 0.088 - ETA: 8:33 - loss: 0.088 - ETA: 8:28 - loss: 0.088 - ETA: 8:22 - loss: 0.088 - ETA: 8:16 - loss: 0.088 - ETA: 8:11 - loss: 0.088 - ETA: 8:05 - loss: 0.088 - ETA: 8:00 - loss: 0.088 - ETA: 7:54 - loss: 0.088 - ETA: 7:49 - loss: 0.088 - ETA: 7:43 - loss: 0.088 - ETA: 7:37 - loss: 0.088 - ETA: 7:32 - loss: 0.088 - ETA: 7:26 - loss: 0.088 - ETA: 7:21 - loss: 0.088 - ETA: 7:15 - loss: 0.088 - ETA: 7:10 - loss: 0.088 - ETA: 7:04 - loss: 0.088 - ETA: 6:58 - loss: 0.088 - ETA: 6:53 - loss: 0.088 - ETA: 6:47 - loss: 0.088 - ETA: 6:42 - loss: 0.088 - ETA: 6:36 - loss: 0.088 - ETA: 6:31 - loss: 0.088 - ETA: 6:25 - loss: 0.088 - ETA: 6:19 - loss: 0.088 - ETA: 6:14 - loss: 0.088 - ETA: 6:08 - loss: 0.088 - ETA: 6:03 - loss: 0.088 - ETA: 5:57 - loss: 0.088 - ETA: 5:52 - loss: 0.088 - ETA: 5:46 - loss: 0.088 - ETA: 5:41 - loss: 0.088 - ETA: 5:35 - loss: 0.088 - ETA: 5:29 - loss: 0.088 - ETA: 5:24 - loss: 0.088 - ETA: 5:18 - loss: 0.088 - ETA: 5:13 - loss: 0.088 - ETA: 5:07 - loss: 0.088 - ETA: 5:02 - loss: 0.088 - ETA: 4:56 - loss: 0.087 - ETA: 4:50 - loss: 0.087 - ETA: 4:45 - loss: 0.087 - ETA: 4:39 - loss: 0.087 - ETA: 4:34 - loss: 0.0879154783/154783 [==============================] - ETA: 4:28 - loss: 0.087 - ETA: 4:23 - loss: 0.087 - ETA: 4:17 - loss: 0.087 - ETA: 4:12 - loss: 0.087 - ETA: 4:06 - loss: 0.087 - ETA: 4:00 - loss: 0.087 - ETA: 3:55 - loss: 0.087 - ETA: 3:49 - loss: 0.087 - ETA: 3:44 - loss: 0.087 - ETA: 3:38 - loss: 0.087 - ETA: 3:33 - loss: 0.087 - ETA: 3:27 - loss: 0.087 - ETA: 3:22 - loss: 0.087 - ETA: 3:16 - loss: 0.087 - ETA: 3:10 - loss: 0.087 - ETA: 3:05 - loss: 0.087 - ETA: 2:59 - loss: 0.087 - ETA: 2:54 - loss: 0.087 - ETA: 2:48 - loss: 0.087 - ETA: 2:43 - loss: 0.087 - ETA: 2:37 - loss: 0.087 - ETA: 2:32 - loss: 0.087 - ETA: 2:26 - loss: 0.087 - ETA: 2:21 - loss: 0.087 - ETA: 2:15 - loss: 0.087 - ETA: 2:09 - loss: 0.087 - ETA: 2:04 - loss: 0.087 - ETA: 1:58 - loss: 0.087 - ETA: 1:53 - loss: 0.087 - ETA: 1:47 - loss: 0.087 - ETA: 1:42 - loss: 0.087 - ETA: 1:36 - loss: 0.087 - ETA: 1:31 - loss: 0.087 - ETA: 1:25 - loss: 0.087 - ETA: 1:20 - loss: 0.087 - ETA: 1:14 - loss: 0.087 - ETA: 1:09 - loss: 0.087 - ETA: 1:03 - loss: 0.087 - ETA: 57s - loss: 0.087 - ETA: 52s - loss: 0.08 - ETA: 46s - loss: 0.08 - ETA: 41s - loss: 0.08 - ETA: 35s - loss: 0.08 - ETA: 30s - loss: 0.08 - ETA: 24s - loss: 0.08 - ETA: 19s - loss: 0.08 - ETA: 13s - loss: 0.08 - ETA: 8s - loss: 0.0875 - ETA: 2s - loss: 0.087 - 13481s 87ms/step - loss: 0.0875 - val_loss: 0.0601\n",
      "Epoch end 1\n",
      " - AUC - improved from 0.00000 to 0.95649\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adam')\n",
    "\n",
    "# Set variables\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "\n",
    "# Set early stopping\n",
    "early_stop = EarlyStopping(monitor=\"roc_auc_val\", mode=\"max\", patience=2)\n",
    "                                                    \n",
    "# Train\n",
    "graph = model.fit(X, y, batch_size=batch_size, epochs=epochs,\n",
    "                  validation_data=(X_val, y_val), callbacks=[RocAuc, early_stop],\n",
    "                  verbose=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFgeJmZll+f8aBEylLZKrCQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x212dccbc400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Visualize history of loss\n",
    "plt.plot(graph.history['loss'])\n",
    "plt.plot(graph.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load best weights and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(processed_X_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle competition submission format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
    "sample_submission[list_classes] = predictions\n",
    "sample_submission.to_csv(\"submission3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### App prediction\n",
    "An app pipeline that can be put into production for toxic comment classification. It will take in a string and return the odds that it is any one of the toxic classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity levels for 'go jump off a bridge jerk':\n",
      "Toxic:         82%\n",
      "Severe Toxic:  4%\n",
      "Obscene:       59%\n",
      "Threat:        0%\n",
      "Insult:        58%\n",
      "Identity Hate: 11%\n",
      "\n",
      "Toxicity levels for 'i will kill you':\n",
      "Toxic:         75%\n",
      "Severe Toxic:  11%\n",
      "Obscene:       37%\n",
      "Threat:        33%\n",
      "Insult:        40%\n",
      "Identity Hate: 15%\n",
      "\n",
      "Toxicity levels for 'have a nice day':\n",
      "Toxic:         2%\n",
      "Severe Toxic:  0%\n",
      "Obscene:       0%\n",
      "Threat:        0%\n",
      "Insult:        0%\n",
      "Identity Hate: 0%\n",
      "\n",
      "Toxicity levels for 'hola, como estas':\n",
      "Toxic:         22%\n",
      "Severe Toxic:  2%\n",
      "Obscene:       12%\n",
      "Threat:        1%\n",
      "Insult:        7%\n",
      "Identity Hate: 3%\n",
      "\n",
      "Toxicity levels for 'hola mierda joder':\n",
      "Toxic:         25%\n",
      "Severe Toxic:  0%\n",
      "Obscene:       6%\n",
      "Threat:        1%\n",
      "Insult:        10%\n",
      "Identity Hate: 2%\n",
      "\n",
      "Toxicity levels for 'fuck off!!':\n",
      "Toxic:         100%\n",
      "Severe Toxic:  23%\n",
      "Obscene:       99%\n",
      "Threat:        0%\n",
      "Insult:        83%\n",
      "Identity Hate: 2%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def toxicity_level(string):\n",
    "    \"\"\"\n",
    "    Return toxicity probability based on inputed string.\n",
    "    \"\"\"\n",
    "    # Process string\n",
    "    new_string = [string]\n",
    "    new_string = tokenizer.texts_to_sequences(new_string)\n",
    "    new_string = pad_sequences(new_string, maxlen=max_len, padding='post', truncating='post')\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(new_string)\n",
    "    \n",
    "    # Print output\n",
    "    print(\"Toxicity levels for '{}':\".format(string))\n",
    "    print('Toxic:         {:.0%}'.format(prediction[0][0]))\n",
    "    print('Severe Toxic:  {:.0%}'.format(prediction[0][1]))\n",
    "    print('Obscene:       {:.0%}'.format(prediction[0][2]))\n",
    "    print('Threat:        {:.0%}'.format(prediction[0][3]))\n",
    "    print('Insult:        {:.0%}'.format(prediction[0][4]))\n",
    "    print('Identity Hate: {:.0%}'.format(prediction[0][5]))\n",
    "    print()\n",
    "    \n",
    "    return\n",
    "\n",
    "toxicity_level('go jump off a bridge jerk')\n",
    "toxicity_level('i will kill you')\n",
    "toxicity_level('have a nice day')\n",
    "toxicity_level('hola, como estas')\n",
    "toxicity_level('hola mierda joder')\n",
    "toxicity_level('fuck off!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above sample data, it can be seen that the predictions are not much accurate.\n",
    "This model is not able to predict the probability as accurate as CNN with word embeddings.\n",
    "This information is not available on Google's Perspective API model where it can be subcatogerized as a threat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity levels for 'Whats up':\n",
      "Toxic:         31%\n",
      "Severe Toxic:  0%\n",
      "Obscene:       11%\n",
      "Threat:        0%\n",
      "Insult:        9%\n",
      "Identity Hate: 0%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "toxicity_level('Whats up')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same data on Google's Perspective API gives a toxicity level of 5% while the above model give a toxicity level of 31%"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "REFERENCES\n",
    "[1] https://www.kaggle.com/c/jigsaw-toxic-comment- classification-challenge#description\n",
    "[2] https://motherboard.vice.com/en_us/article/qvvv3p/ googles-anti-bullying-ai-mistakes-civility-for- decency\n",
    "[3] http://colah.github.io/posts/2015-08-Understanding- LSTMs/\n",
    "[4] http://nymag.com/selectall/2017/02/google-introduces- perspective-a-tool-for-toxic-comments.html\n",
    "[5] https://web.stanford.edu/class/cs224n/reports/2762092.pdf\n",
    "[6] https://www.kaggle.com/sbongo/for-beginners-tackling-toxic-using-keras\n",
    "[7] https://datascience.stackexchange.com/questions/11619/rnn-vs-cnn-at-a-high-level\n",
    "[8] https://www.depends-on-the-definition.com/classify-toxic-comments-on-wikipedia\n",
    "[9] http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp\n",
    "[10] http://web.stanford.edu/class/cs224n/reports/6838795.pdf\n",
    "[11] http://dsbyprateekg.blogspot.com/2017/12/can-you-build-model-to-predict-toxic.html\n",
    "[12] https://arxiv.org/pdf/1802.09957.pdf\n",
    "[13] http://web.stanford.edu/class/cs224n/reports/6909170.pdf\n",
    "[14] http://web.stanford.edu/class/cs224n/reports/6838601.pdf\n",
    "[15] http://web.stanford.edu/class/cs224n/reports.html\n",
    "[16] https://medium.com/@srjoglekar246/first-time-with-kaggle-a-convnet-to-classify-toxic-comments-with-keras-ef84b6d18328"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
